import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@/lib/supabase/server'
import { isDevMode, DEV_USER } from '@/lib/dev-user'

/**
 * ğŸ”­ ë·°íŒŒì¸ë” (Viewfinder) API
 *
 * AI ì—ì´ì „íŠ¸ê°€ ì´ë¯¸ì§€/ë¬¸ì„œë¥¼ "ëˆˆìœ¼ë¡œ ë³´ê³ " ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë„êµ¬
 * - ì´ë¯¸ì§€ URL ë˜ëŠ” base64 ì§€ì›
 * - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì‚¬ìš© (GPT-4o, Gemini, Claude)
 * - íšŒì˜ì—ì„œ ê³µìœ ëœ ìë£Œ ë¶„ì„ìš©
 */

interface ViewfinderRequest {
  imageUrl?: string           // ì´ë¯¸ì§€ URL
  imageBase64?: string        // Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€
  mimeType?: string           // image/png, image/jpeg, application/pdf ë“±
  prompt?: string             // ë¶„ì„ ìš”ì²­ (ì˜ˆ: "ì´ ì°¨íŠ¸ë¥¼ ë¶„ì„í•´ì¤˜")
  provider?: 'openai' | 'gemini' | 'anthropic'  // ì–´ë–¤ ë¹„ì „ ëª¨ë¸ ì‚¬ìš©
  agentName?: string          // ëˆ„ê°€ ë³´ê³  ìˆëŠ”ì§€ (íšŒì˜ ì»¨í…ìŠ¤íŠ¸ìš©)
}

export async function POST(request: NextRequest) {
  const supabase = await createClient()

  try {
    // ì¸ì¦ í™•ì¸
    let user: any = isDevMode() ? DEV_USER : null
    if (!user) {
      const { data } = await supabase.auth.getUser()
      user = data.user
    }

    if (!user) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    const body: ViewfinderRequest = await request.json()
    const {
      imageUrl,
      imageBase64,
      mimeType = 'image/png',
      prompt = 'ì´ ì´ë¯¸ì§€/ë¬¸ì„œë¥¼ ë¶„ì„í•´ì„œ ì£¼ìš” ë‚´ìš©ì„ ì„¤ëª…í•´ì¤˜.',
      provider = 'openai',  // GPT-4oê°€ ê°€ì¥ ì•ˆì •ì 
      agentName,
    } = body

    if (!imageUrl && !imageBase64) {
      return NextResponse.json(
        { error: 'imageUrl ë˜ëŠ” imageBase64ê°€ í•„ìš”í•©ë‹ˆë‹¤.' },
        { status: 400 }
      )
    }

    console.log('[Viewfinder] ğŸ”­ Vision analysis request:', {
      provider,
      hasUrl: !!imageUrl,
      hasBase64: !!imageBase64,
      mimeType,
      agentName,
    })

    let analysis = ''

    // ğŸ”¥ OpenAI GPT-4o Vision
    if (provider === 'openai') {
      const OpenAI = (await import('openai')).default
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

      const imageContent = imageUrl
        ? { type: 'image_url' as const, image_url: { url: imageUrl } }
        : { type: 'image_url' as const, image_url: { url: `data:${mimeType};base64,${imageBase64}` } }

      const systemPrompt = agentName
        ? `ë„ˆëŠ” ${agentName}ì•¼. íšŒì˜ ì¤‘ ê³µìœ ëœ ìë£Œë¥¼ ë¶„ì„í•˜ê³  ìˆì–´. íŒ€ì›ë“¤ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì¤˜.`
        : 'ì´ë¯¸ì§€/ë¬¸ì„œë¥¼ ë¶„ì„í•´ì„œ í•µì‹¬ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.'

      const response = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          { role: 'system', content: systemPrompt },
          {
            role: 'user',
            content: [
              { type: 'text', text: prompt },
              imageContent,
            ],
          },
        ],
        max_tokens: 1000,
      })

      analysis = response.choices[0]?.message?.content || 'ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'
    }

    // ğŸ”¥ Google Gemini Vision
    else if (provider === 'gemini') {
      const { GoogleGenerativeAI } = await import('@google/generative-ai')
      const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')
      const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash-exp' })

      const systemPrompt = agentName
        ? `ë„ˆëŠ” ${agentName}ì•¼. íšŒì˜ ì¤‘ ê³µìœ ëœ ìë£Œë¥¼ ë¶„ì„í•˜ê³  ìˆì–´.`
        : ''

      let imageParts: any[] = []

      if (imageBase64) {
        imageParts = [{
          inlineData: {
            data: imageBase64,
            mimeType: mimeType,
          },
        }]
      } else if (imageUrl) {
        // URLì¸ ê²½ìš° fetchí•´ì„œ base64ë¡œ ë³€í™˜
        const imageResponse = await fetch(imageUrl)
        const buffer = await imageResponse.arrayBuffer()
        const base64 = Buffer.from(buffer).toString('base64')
        const contentType = imageResponse.headers.get('content-type') || mimeType

        imageParts = [{
          inlineData: {
            data: base64,
            mimeType: contentType,
          },
        }]
      }

      const result = await model.generateContent([
        `${systemPrompt}\n\n${prompt}`,
        ...imageParts,
      ])

      analysis = result.response.text() || 'ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'
    }

    // ğŸ”¥ Anthropic Claude Vision
    else if (provider === 'anthropic') {
      const Anthropic = (await import('@anthropic-ai/sdk')).default
      const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

      const systemPrompt = agentName
        ? `ë„ˆëŠ” ${agentName}ì•¼. íšŒì˜ ì¤‘ ê³µìœ ëœ ìë£Œë¥¼ ë¶„ì„í•˜ê³  ìˆì–´.`
        : 'ì´ë¯¸ì§€/ë¬¸ì„œë¥¼ ë¶„ì„í•´ì„œ í•µì‹¬ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.'

      let imageData: any

      if (imageBase64) {
        imageData = {
          type: 'base64',
          media_type: mimeType,
          data: imageBase64,
        }
      } else if (imageUrl) {
        // ClaudeëŠ” URLë„ ì§€ì›í•˜ì§€ë§Œ base64ê°€ ë” ì•ˆì •ì 
        const imageResponse = await fetch(imageUrl)
        const buffer = await imageResponse.arrayBuffer()
        const base64 = Buffer.from(buffer).toString('base64')
        const contentType = imageResponse.headers.get('content-type') || mimeType

        imageData = {
          type: 'base64',
          media_type: contentType,
          data: base64,
        }
      }

      const response = await anthropic.messages.create({
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 1000,
        system: systemPrompt,
        messages: [
          {
            role: 'user',
            content: [
              { type: 'image', source: imageData },
              { type: 'text', text: prompt },
            ],
          },
        ],
      })

      const textBlock = response.content.find(block => block.type === 'text')
      analysis = textBlock?.type === 'text' ? textBlock.text : 'ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'
    }

    console.log('[Viewfinder] âœ… Analysis complete:', {
      provider,
      agentName,
      analysisLength: analysis.length,
    })

    return NextResponse.json({
      success: true,
      analysis,
      provider,
      agentName,
    })

  } catch (error: any) {
    console.error('[Viewfinder] âŒ Error:', error)
    return NextResponse.json(
      { error: error.message || 'Vision analysis failed' },
      { status: 500 }
    )
  }
}
