"use client"

/**
 * SpeakerMode - íšŒì˜/í† ë¡ ìš© ìŠ¤í”¼ì»¤ ëª¨ë“œ
 *
 * ğŸ”Š ìƒˆë¡œìš´ ì‹¬í”Œí•œ ë°©ì‹:
 * 1. ê¸°ì¡´ í…ìŠ¤íŠ¸ ì±„íŒ… ê·¸ëŒ€ë¡œ ìœ ì§€
 * 2. ìŠ¤í”¼ì»¤ ON â†’ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ê°€ TTSë¡œ ì½í˜€ì§
 * 3. ë§ˆì´í¬ ON â†’ ì‚¬ìš©ì ìŒì„±ì´ STTë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ë˜ì–´ ì±„íŒ…ì— ì…ë ¥
 *
 * ì¥ì : ë³µì¡í•œ í„´ ê´€ë¦¬ ì—†ìŒ, ê¸°ì¡´ ë¦´ë ˆì´ ë¡œì§ ì¬ì‚¬ìš©
 */

import { useState, useRef, useCallback, useEffect } from "react"
import { Volume2, VolumeX, Mic, MicOff, Loader2 } from "lucide-react"

// ì—ì´ì „íŠ¸ë³„ ìŒì„± ë§¤í•‘ (OpenAI TTS voices: nova, shimmer, echo, onyx, fable, alloy, ash, sage, coral)
const AGENT_VOICES: Record<string, string> = {
  // ì—¬ì„± (nova, shimmer, coral, sage)
  "ë ˆì´ì²¼": "nova",
  "rachel": "nova",
  "ì—ì´ë¯¸": "shimmer",
  "amy": "shimmer",
  "ì†Œí”¼ì•„": "coral",
  "sophia": "coral",
  // ë‚¨ì„± (echo, onyx, fable, ash)
  "ì œë ˆë¯¸": "echo",
  "jeremy": "echo",
  "ë§ˆì´í´": "onyx",
  "michael": "onyx",
  // ê¸°ë³¸
  "default": "alloy"
}

function getVoiceForAgent(name: string): string {
  const nameLower = name.toLowerCase()
  return AGENT_VOICES[name] || AGENT_VOICES[nameLower] || AGENT_VOICES.default
}

interface SpeakerModeProps {
  /** ìŠ¤í”¼ì»¤ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ */
  enabled: boolean
  /** ìŠ¤í”¼ì»¤ ëª¨ë“œ í† ê¸€ */
  onToggle: (enabled: boolean) => void
  /** ìŒì„± ì…ë ¥ ê²°ê³¼ (STT ì™„ë£Œ ì‹œ) */
  onVoiceInput?: (text: string) => void
  /** í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ì—ì´ì „íŠ¸ ì´ë¦„ */
  currentSpeaker?: string | null
}

export function SpeakerMode({
  enabled,
  onToggle,
  onVoiceInput,
  currentSpeaker,
}: SpeakerModeProps) {
  const [isListening, setIsListening] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [transcript, setTranscript] = useState("")

  // ë§ˆì´í¬ ê´€ë ¨
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])

  // ğŸ¤ ìŒì„± ë…¹ìŒ ì‹œì‘
  const startListening = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
        }
      })

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })
      mediaRecorderRef.current = mediaRecorder
      audioChunksRef.current = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        stream.getTracks().forEach(track => track.stop())

        if (audioChunksRef.current.length === 0) return

        // STT ì²˜ë¦¬
        setIsProcessing(true)
        try {
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })
          const formData = new FormData()
          formData.append('audio', audioBlob, 'recording.webm')

          const response = await fetch('/api/voice/stt', {
            method: 'POST',
            body: formData,
          })

          if (response.ok) {
            const { text } = await response.json()
            if (text?.trim()) {
              setTranscript(text)
              onVoiceInput?.(text)
            }
          }
        } catch (error) {
          console.error('[SpeakerMode] STT error:', error)
        } finally {
          setIsProcessing(false)
          setTranscript("")
        }
      }

      mediaRecorder.start()
      setIsListening(true)
      console.log('[SpeakerMode] ğŸ¤ Listening started')
    } catch (error) {
      console.error('[SpeakerMode] Mic access error:', error)
    }
  }, [onVoiceInput])

  // ğŸ¤ ìŒì„± ë…¹ìŒ ì¤‘ì§€
  const stopListening = useCallback(() => {
    if (mediaRecorderRef.current && isListening) {
      mediaRecorderRef.current.stop()
      mediaRecorderRef.current = null
      setIsListening(false)
      console.log('[SpeakerMode] ğŸ¤ Listening stopped')
    }
  }, [isListening])

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (mediaRecorderRef.current) {
        mediaRecorderRef.current.stop()
      }
    }
  }, [])

  return (
    <div className="flex items-center gap-2">
      {/* ğŸ”Š ìŠ¤í”¼ì»¤ í† ê¸€ */}
      <button
        onClick={() => onToggle(!enabled)}
        className={`p-2 rounded-lg transition-all ${
          enabled
            ? "bg-emerald-500/20 text-emerald-400 hover:bg-emerald-500/30"
            : "bg-zinc-800 text-zinc-400 hover:bg-zinc-700"
        }`}
        title={enabled ? "ìŠ¤í”¼ì»¤ ëª¨ë“œ ë„ê¸°" : "ìŠ¤í”¼ì»¤ ëª¨ë“œ ì¼œê¸°"}
      >
        {enabled ? <Volume2 className="w-5 h-5" /> : <VolumeX className="w-5 h-5" />}
      </button>

      {/* ğŸ¤ ë§ˆì´í¬ ë²„íŠ¼ (ìŠ¤í”¼ì»¤ ëª¨ë“œì¼ ë•Œë§Œ) */}
      {enabled && (
        <button
          onClick={isListening ? stopListening : startListening}
          disabled={isProcessing}
          className={`p-2 rounded-lg transition-all ${
            isListening
              ? "bg-red-500/20 text-red-400 animate-pulse"
              : isProcessing
                ? "bg-cyan-500/20 text-cyan-400"
                : "bg-zinc-800 text-zinc-400 hover:bg-zinc-700"
          }`}
          title={isListening ? "ë…¹ìŒ ì¤‘ì§€" : "ìŒì„±ìœ¼ë¡œ ë§í•˜ê¸°"}
        >
          {isProcessing ? (
            <Loader2 className="w-5 h-5 animate-spin" />
          ) : isListening ? (
            <Mic className="w-5 h-5" />
          ) : (
            <MicOff className="w-5 h-5" />
          )}
        </button>
      )}

      {/* í˜„ì¬ ë§í•˜ëŠ” ì—ì´ì „íŠ¸ í‘œì‹œ */}
      {enabled && currentSpeaker && (
        <div className="flex items-center gap-1.5 px-2 py-1 bg-purple-500/20 rounded-md">
          <div className="flex gap-0.5">
            {[0, 1, 2].map(i => (
              <div
                key={i}
                className="w-1 h-3 bg-purple-400 rounded-full animate-pulse"
                style={{ animationDelay: `${i * 0.15}s` }}
              />
            ))}
          </div>
          <span className="text-xs text-purple-300">{currentSpeaker}</span>
        </div>
      )}

      {/* ë…¹ìŒ ì¤‘ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë¯¸ë¦¬ë³´ê¸° */}
      {isListening && (
        <span className="text-xs text-zinc-500">ë…¹ìŒ ì¤‘...</span>
      )}
    </div>
  )
}

/**
 * ğŸ”Š ì—ì´ì „íŠ¸ ë©”ì‹œì§€ TTS ì¬ìƒ í›…
 *
 * ëŠê¹€ ë°©ì§€ ëª¨ë“œ:
 * - í˜„ì¬ ì¬ìƒ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (ëŠì§€ ì•ŠìŒ)
 * - ëŒ€ê¸° ì¤‘ ìƒˆ ë©”ì‹œì§€ ì˜¤ë©´ ìµœì‹  ê²ƒë§Œ ì €ì¥
 * - ì¬ìƒ ëë‚˜ë©´ ì €ì¥ëœ ìµœì‹  ë©”ì‹œì§€ ì¬ìƒ
 */
export function useSpeakerTTS() {
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [currentSpeaker, setCurrentSpeaker] = useState<string | null>(null)
  const currentAudioRef = useRef<HTMLAudioElement | null>(null)
  const isPlayingRef = useRef(false)
  const pendingRef = useRef<{ text: string; agentName: string } | null>(null)

  // TTS ì¬ìƒ (ëŠê¹€ ë°©ì§€)
  const playTTS = useCallback(async (text: string, agentName: string) => {
    // ì¬ìƒ ì¤‘ì´ë©´ ëŒ€ê¸°ì—´ì— ì €ì¥ (ìµœì‹  ê²ƒë§Œ)
    if (isPlayingRef.current) {
      pendingRef.current = { text, agentName }
      console.log('[TTS] â³ ëŒ€ê¸°:', agentName)
      return
    }

    const play = async (msg: { text: string; agentName: string }) => {
      const voice = getVoiceForAgent(msg.agentName)
      isPlayingRef.current = true
      setIsSpeaking(true)
      setCurrentSpeaker(msg.agentName)

      try {
        const res = await fetch('/api/voice/tts', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: msg.text, voice, model: 'tts-1' }),
        })

        if (res.ok) {
          const blob = await res.blob()
          const url = URL.createObjectURL(blob)
          const audio = new Audio(url)
          currentAudioRef.current = audio

          await new Promise<void>((resolve) => {
            audio.onended = () => { URL.revokeObjectURL(url); resolve() }
            audio.onerror = () => { URL.revokeObjectURL(url); resolve() }
            audio.play().catch(resolve)
          })
        }
      } catch (e) {
        console.error('[TTS] Error:', e)
      }

      currentAudioRef.current = null
      isPlayingRef.current = false

      // ëŒ€ê¸° ë©”ì‹œì§€ ìˆìœ¼ë©´ ì¬ìƒ
      if (pendingRef.current) {
        const next = pendingRef.current
        pendingRef.current = null
        console.log('[TTS] â–¶ï¸ ë‹¤ìŒ:', next.agentName)
        await play(next)
      } else {
        setIsSpeaking(false)
        setCurrentSpeaker(null)
      }
    }

    console.log('[TTS] â–¶ï¸ ì¬ìƒ:', agentName)
    await play({ text, agentName })
  }, [])

  // ì¤‘ë‹¨
  const clearQueue = useCallback(() => {
    if (currentAudioRef.current) {
      currentAudioRef.current.pause()
      currentAudioRef.current.src = ''
      currentAudioRef.current = null
    }
    pendingRef.current = null
    isPlayingRef.current = false
    setIsSpeaking(false)
    setCurrentSpeaker(null)
  }, [])

  return {
    playTTS,
    clearQueue,
    isSpeaking,
    currentSpeaker,
  }
}

export default SpeakerMode
