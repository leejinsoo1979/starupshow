"use client"

/**
 * MeetingVoiceChat - íšŒì˜ì‹¤ ë‹¤ìê°„ ìŒì„± ì±„íŒ… (ì§„ì§œ í‹°í‚¤íƒ€ì¹´ ë²„ì „)
 *
 * ğŸ”¥ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜:
 * 1. ì‚¬ìš©ì ë°œí™” â†’ xAI Grokìœ¼ë¡œ STT (ìŒì„±â†’í…ìŠ¤íŠ¸)
 * 2. ê° ì—ì´ì „íŠ¸ë³„ ê°œë³„ Chat API í˜¸ì¶œ (ì™„ì „í•œ í˜ë¥´ì†Œë‚˜ ìœ ì§€)
 * 3. ê° ì—ì´ì „íŠ¸ë³„ TTSë¡œ ìŒì„± ì‘ë‹µ (ê°ì ë‹¤ë¥¸ ëª©ì†Œë¦¬)
 * 4. ìˆœì°¨ì  ì‘ë‹µìœ¼ë¡œ ì§„ì§œ íšŒì˜ì²˜ëŸ¼ ëŒ€í™”
 */

import { useState, useRef, useCallback, useEffect } from "react"
import { motion, AnimatePresence } from "framer-motion"
import { Mic, MicOff, Phone, PhoneOff, Volume2, VolumeX, Loader2, Users, Hand, AlertCircle, MessageCircle, Eye, Upload, X, Image as ImageIcon, ChevronDown, ChevronUp, GripVertical, Globe } from "lucide-react"
import { AIViewfinder, ViewfinderCaptureResult } from "@/components/neural-map/viewfinder/AIViewfinder"

// ì°¸ì—¬ì íƒ€ì…
interface VoiceParticipant {
  id: string
  name: string
  type: "user" | "agent"
  avatarUrl?: string
  voice?: "sol" | "tara" | "cove" | "puck" | "charon" | "vale" | "alloy" | "echo" | "fable" | "onyx" | "nova" | "shimmer"
  role?: string
  color?: string
  systemPrompt?: string  // ğŸ”¥ ê° ì—ì´ì „íŠ¸ì˜ ì™„ì „í•œ í˜ë¥´ì†Œë‚˜
}

// ìŒì„± ë©”ì‹œì§€
interface VoiceMessage {
  id: string
  participantId: string
  participantName: string
  participantType: "user" | "agent"
  text: string
  timestamp: Date
  isComplete: boolean
}

// ğŸ”¥ ê³µìœ ëœ ìë£Œ (ì´ë¯¸ì§€, ë¬¸ì„œ ë“±)
interface SharedDocument {
  id: string
  name: string
  type: 'image' | 'pdf' | 'document' | 'url'
  content: string  // base64 ë˜ëŠ” URL
  mimeType?: string
  analysis?: string  // AIê°€ ë¶„ì„í•œ ë‚´ìš©
  timestamp: Date
}

interface MeetingVoiceChatProps {
  roomId: string
  participants: VoiceParticipant[]
  currentUserId: string
  currentUserName: string
  onMessage?: (message: VoiceMessage) => void
  onTranscript?: (text: string, participantId: string) => void
  meetingTopic?: string
  /** ğŸ”¥ ê³µìœ ëœ ìë£Œë“¤ (ì´ë¯¸ì§€, ë¬¸ì„œ ë“±) */
  sharedDocuments?: SharedDocument[]
  /** ğŸ”¥ ìë£Œ ë¶„ì„ ì™„ë£Œ ì½œë°± */
  onDocumentAnalyzed?: (docId: string, analysis: string) => void
  /** ğŸ”­ íŒŒì¼ ê³µìœ  ì½œë°± (ë·°íŒŒì¸ë” ìº¡ì²˜ ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œ) */
  onShareFile?: (file: File | { dataUrl: string; name: string; type: string }) => void
  /** ğŸŒ ë¸Œë¼ìš°ì € ì—´ê¸° ì½œë°± */
  onOpenBrowser?: () => void
}

type ConnectionStatus = "disconnected" | "connecting" | "connected" | "error"
type SpeakingStatus = "idle" | "listening" | "speaking" | "waiting"

// ì—­í• ë³„ ê¸°ë³¸ ìŒì„± ë§¤í•‘ (Grok ìŒì„±ë§Œ ì‚¬ìš©)
// Grok ìŒì„±: sol(ì°¨ë¶„ ì—¬ì„±), tara(í™œê¸° ì—¬ì„±), cove(ë”°ëœ» ë‚¨ì„±), puck(ìœ ì¾Œ ë‚¨ì„±), charon(ê¹Šì€ ë‚¨ì„±), vale(ì¤‘ì„±)
const ROLE_VOICE_MAP: Record<string, string> = {
  strategist: "charon",
  analyst: "sol",
  executor: "cove",
  critic: "puck",
  mediator: "vale",
  default: "tara",
}

// ì—ì´ì „íŠ¸ ì´ë¦„ë³„ ìŒì„± ë§¤í•‘ (Grok ìŒì„±ë§Œ ì‚¬ìš©)
const AGENT_NAME_VOICE_MAP: Record<string, string> = {
  // ì—¬ì„± ì—ì´ì „íŠ¸
  "ì—ì´ë¯¸": "tara",
  "amy": "tara",
  "ë ˆì´ì²¼": "sol",
  "rachel": "sol",
  "ì†Œí”¼ì•„": "tara",
  "sophia": "tara",
  "ì• ë‹ˆ": "vale",
  "ani": "vale",
  // ë‚¨ì„± ì—ì´ì „íŠ¸
  "ì œë ˆë¯¸": "charon",
  "jeremy": "charon",
  "ë§ˆì´í´": "cove",
  "michael": "cove",
}

// ì—ì´ì „íŠ¸ì˜ ìŒì„± ê²°ì • (ì´ë¦„ > ì—­í•  > ê¸°ë³¸ê°’)
function getVoiceForAgent(agent: VoiceParticipant): string {
  // 1. ì—ì´ì „íŠ¸ propsì— voiceê°€ ì§ì ‘ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©
  if (agent.voice) return agent.voice

  // 2. ì—ì´ì „íŠ¸ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
  const nameLower = agent.name.toLowerCase()
  if (AGENT_NAME_VOICE_MAP[agent.name]) return AGENT_NAME_VOICE_MAP[agent.name]
  if (AGENT_NAME_VOICE_MAP[nameLower]) return AGENT_NAME_VOICE_MAP[nameLower]

  // 3. ì—­í• ë¡œ ë§¤í•‘
  if (agent.role && ROLE_VOICE_MAP[agent.role]) return ROLE_VOICE_MAP[agent.role]

  // 4. ê¸°ë³¸ê°’
  return ROLE_VOICE_MAP.default
}

export function MeetingVoiceChat({
  roomId,
  participants,
  currentUserId,
  currentUserName,
  onMessage,
  onTranscript,
  meetingTopic = "íšŒì˜",
  sharedDocuments = [],
  onDocumentAnalyzed,
  onShareFile,
  onOpenBrowser,
}: MeetingVoiceChatProps) {
  // ì—°ê²° ìƒíƒœ
  const [status, setStatus] = useState<ConnectionStatus>("disconnected")
  const [isMuted, setIsMuted] = useState(false)
  const [isSpeakerMuted, setIsSpeakerMuted] = useState(false)

  // ğŸ”­ ë·°íŒŒì¸ë” ìƒíƒœ
  const [showViewfinder, setShowViewfinder] = useState(false)
  const fileInputRef = useRef<HTMLInputElement>(null)

  // ğŸ“‚ ìë£Œê³µìœ ì°½ í´ë”© ìƒíƒœ
  const [isDocsCollapsed, setIsDocsCollapsed] = useState(false)

  // ğŸ“ íŒ¨ë„ ë¦¬ì‚¬ì´ì¦ˆ ìƒíƒœ
  const [panelWidth, setPanelWidth] = useState(320) // ê¸°ë³¸ 320px
  const isResizingRef = useRef(false)
  const resizeStartXRef = useRef(0)
  const resizeStartWidthRef = useRef(0)
  const panelRef = useRef<HTMLDivElement>(null)

  // ì°¨ë¡€ ê´€ë¦¬
  const [currentSpeaker, setCurrentSpeaker] = useState<string | null>(null)
  const [speakingStatus, setSpeakingStatus] = useState<SpeakingStatus>("idle")
  const [turnQueue, setTurnQueue] = useState<string[]>([])
  const [handRaised, setHandRaised] = useState(false)

  // ë©”ì‹œì§€ ë° íŠ¸ëœìŠ¤í¬ë¦½íŠ¸
  const [messages, setMessages] = useState<VoiceMessage[]>([])
  const [currentTranscript, setCurrentTranscript] = useState<string>("")
  const [agentResponses, setAgentResponses] = useState<Map<string, string>>(new Map())

  // ì˜¤ë””ì˜¤ ê´€ë ¨ refs
  const wsRef = useRef<WebSocket | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const processorRef = useRef<ScriptProcessorNode | null>(null)
  const audioQueueRef = useRef<Float32Array[]>([])
  const isPlayingRef = useRef(false)
  const turnLockRef = useRef(false)

  // AI ì—ì´ì „íŠ¸ ì‘ë‹µ í (ì§„ì§œ ë©€í‹° ì—ì´ì „íŠ¸ í„´)
  const agentResponseQueueRef = useRef<{ agentId: string; text: string; voice: string }[]>([])
  const isAgentSpeakingRef = useRef(false)
  const agentContextsRef = useRef<Map<string, { prompt: string; voice: string; chatHistory: any[] }>>(new Map())
  const isProcessingResponseRef = useRef(false)

  // ì°¸ì—¬ì ì¤‘ AI ì—ì´ì „íŠ¸ë§Œ í•„í„°ë§
  const agentParticipants = participants.filter(p => p.type === "agent")
  const userParticipants = participants.filter(p => p.type === "user")

  // í˜„ì¬ ì‚¬ìš©ìê°€ ë§í•  ì°¨ë¡€ì¸ì§€ í™•ì¸
  const isMyTurn = currentSpeaker === currentUserId || currentSpeaker === null

  // ğŸ”¥ íšŒì˜ ì „ìš© APIë¡œ ì—ì´ì „íŠ¸ ì‘ë‹µ ìš”ì²­ (ì—ì´ì „íŠ¸ ê°„ ëŒ€í™” ì§€ì›)
  const getAgentResponse = useCallback(async (
    agentId: string,
    agentName: string,
    lastMessage: string,
    lastSpeaker: string
  ): Promise<{ text: string; voice: string } | null> => {
    const agentContext = agentContextsRef.current.get(agentId)
    if (!agentContext) {
      console.warn(`[MeetingVoice] No context for agent: ${agentName}`)
      return null
    }

    try {
      // ğŸ”¥ íšŒì˜ ì „ìš© Chat API í˜¸ì¶œ (ì—ì´ì „íŠ¸ ê°„ ëŒ€í™” ì§€ì›)
      const response = await fetch('/api/meeting/chat', {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          agentId,
          agentName,
          systemPrompt: agentContext.prompt,
          lastSpeaker,                    // ğŸ”¥ ë§ˆì§€ë§‰ ë°œí™”ì (ì‚¬ìš©ì or ë‹¤ë¥¸ ì—ì´ì „íŠ¸)
          lastMessage,                    // ğŸ”¥ ë§ˆì§€ë§‰ ë°œí™” ë‚´ìš©
          conversationHistory: meetingHistoryRef.current,  // ğŸ”¥ ì „ì²´ íšŒì˜ ëŒ€í™”
          meetingTopic,
          participants: [currentUserName, ...agentParticipants.map(a => a.name)],
          sharedDocuments,                // ğŸ”­ ê³µìœ ëœ ë¬¸ì„œë“¤ (ë¹„ì „ ë¶„ì„ ê²°ê³¼ í¬í•¨)
        }),
      })

      if (!response.ok) {
        console.error(`[MeetingVoice] Meeting Chat API failed for ${agentName}`)
        return null
      }

      const data = await response.json()
      const agentText = data.response || ""

      console.log(`[MeetingVoice] ğŸ’¬ ${agentName} responds to ${lastSpeaker}:`, agentText.substring(0, 50))

      return { text: agentText, voice: agentContext.voice }
    } catch (error) {
      console.error(`[MeetingVoice] Error getting response from ${agentName}:`, error)
      return null
    }
  }, [meetingTopic, currentUserName, agentParticipants, sharedDocuments])

  // ğŸ”¥ ì—ì´ì „íŠ¸ ì‘ë‹µì„ TTSë¡œ ì¬ìƒ
  const playAgentResponse = useCallback(async (agentId: string, agentName: string, text: string, voice: string) => {
    if (!text || isSpeakerMuted) return

    try {
      console.log(`[MeetingVoice] ğŸ”Š Playing TTS for ${agentName} with voice: ${voice}`)
      setCurrentSpeaker(agentId)
      setSpeakingStatus("listening")
      isAgentSpeakingRef.current = true

      // xAI Grok TTS API í˜¸ì¶œ
      const ttsResponse = await fetch("/api/voice/grok", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text, voice }),
      })

      if (!ttsResponse.ok) {
        console.error(`[MeetingVoice] TTS failed for ${agentName}`)
        return
      }

      // ì˜¤ë””ì˜¤ ì¬ìƒ
      const audioBlob = await ttsResponse.blob()
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)

      await new Promise<void>((resolve) => {
        audio.onended = () => {
          URL.revokeObjectURL(audioUrl)
          resolve()
        }
        audio.onerror = () => {
          URL.revokeObjectURL(audioUrl)
          resolve()
        }
        audio.play().catch(() => resolve())
      })

      console.log(`[MeetingVoice] âœ… Finished playing ${agentName}'s response`)
    } catch (error) {
      console.error(`[MeetingVoice] TTS error for ${agentName}:`, error)
    } finally {
      isAgentSpeakingRef.current = false
    }
  }, [isSpeakerMuted])

  // ğŸ”¥ íšŒì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì—ì´ì „íŠ¸ ê°„ ëŒ€í™” í¬í•¨)
  const meetingHistoryRef = useRef<{ speaker: string; text: string }[]>([])

  // ğŸ”¥ ì‚¬ìš©ì ë°œí™” í›„ ì—ì´ì „íŠ¸ë“¤ì´ ì„œë¡œ í† ë¡  (ì§„ì§œ í‹°í‚¤íƒ€ì¹´)
  const processAgentResponses = useCallback(async (userMessage: string) => {
    if (isProcessingResponseRef.current) return
    isProcessingResponseRef.current = true

    // ì‚¬ìš©ì ë°œí™”ë¥¼ íšŒì˜ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    meetingHistoryRef.current.push({ speaker: currentUserName, text: userMessage })

    console.log(`[MeetingVoice] ğŸ¯ Starting real discussion for: "${userMessage}"`)

    // ğŸ”­ ê³µìœ ëœ ë¬¸ì„œ ë¶„ì„ ë‚´ìš© ìš”ì•½
    const sharedDocsContext = sharedDocuments
      .filter(doc => doc.analysis)
      .map(doc => `ğŸ“„ ${doc.name}:\n${doc.analysis}`)
      .join('\n\n')

    // ğŸ”¥ ëŒ€í™” í„´ ì‹œìŠ¤í…œ - ì—ì´ì „íŠ¸ë“¤ì´ ì„œë¡œì—ê²Œ ë§í•¨
    const totalTurns = agentParticipants.length * 2  // ê° ì—ì´ì „íŠ¸ 2ë²ˆì”©

    for (let turn = 0; turn < totalTurns; turn++) {
      const agentIndex = turn % agentParticipants.length
      const agent = agentParticipants[agentIndex]
      const otherAgents = agentParticipants.filter(a => a.id !== agent.id)

      setCurrentSpeaker(agent.id)
      setSpeakingStatus("waiting")

      // ğŸ”¥ ë§ˆì§€ë§‰ ë°œí™”ì ì •ë³´
      const lastMessage = meetingHistoryRef.current[meetingHistoryRef.current.length - 1]
      const lastSpeaker = lastMessage?.speaker || currentUserName
      const lastText = lastMessage?.text || userMessage

      // ğŸ”¥ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœê·¼ 5ê°œë§Œ)
      const recentHistory = meetingHistoryRef.current.slice(-5)
        .map(h => `${h.speaker}: ${h.text}`)
        .join('\n')

      // ğŸ”¥ í•µì‹¬: ì´ì „ ë°œí™”ìì—ê²Œ ì§ì ‘ ì‘ë‹µí•˜ë„ë¡ ì§€ì‹œ
      const meetingContext = `## íšŒì˜: ${meetingTopic}
${sharedDocsContext ? `\n## ğŸ”­ ê³µìœ ëœ ìë£Œ (AIê°€ ë¶„ì„í•¨):\n${sharedDocsContext}\n` : ''}
## ë°©ê¸ˆ ${lastSpeaker}ì´(ê°€) ë§í•¨:
"${lastText}"

## ìµœê·¼ ëŒ€í™”:
${recentHistory}

## ğŸ¯ ë‹¹ì‹ ì˜ ì°¨ë¡€ì…ë‹ˆë‹¤, ${agent.name}!
- **${lastSpeaker}ì´(ê°€) í•œ ë§ì— ì§ì ‘ ë°˜ì‘í•˜ì„¸ìš”**
${sharedDocsContext ? '- ê³µìœ ëœ ìë£Œ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ë§í•˜ì„¸ìš”\n' : ''}- ${otherAgents.map(a => a.name).join(', ')}ì—ê²Œ ì§ˆë¬¸í•˜ê±°ë‚˜ ì˜ê²¬ì„ ë¬¼ì–´ë³´ì„¸ìš”
- ì˜ˆ: "${otherAgents[0]?.name || 'ë ˆì´ì²¼'}, ë„Œ ì–´ë–»ê²Œ ìƒê°í•´?"
- ì˜ˆ: "${lastSpeaker} ë§ì— ë™ì˜í•´. ì¶”ê°€ë¡œ..."
- ì§§ê²Œ 1-2ë¬¸ì¥ìœ¼ë¡œ ë§í•˜ì„¸ìš”`

      // ğŸ”¥ ì—ì´ì „íŠ¸ ì‘ë‹µ ìƒì„± (ì´ì „ ë°œí™”ìì—ê²Œ ì‘ë‹µ)
      const response = await getAgentResponse(agent.id, agent.name, lastText, lastSpeaker)

      if (response && response.text) {
        // íšŒì˜ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        meetingHistoryRef.current.push({ speaker: agent.name, text: response.text })

        // ë©”ì‹œì§€ ì €ì¥
        const agentMsg: VoiceMessage = {
          id: `agent-${agent.id}-${Date.now()}`,
          participantId: agent.id,
          participantName: agent.name,
          participantType: "agent",
          text: response.text,
          timestamp: new Date(),
          isComplete: true,
        }
        setMessages(prev => [...prev, agentMsg])
        onMessage?.(agentMsg)
        onTranscript?.(response.text, agent.id)

        // TTSë¡œ ì‘ë‹µ ì¬ìƒ
        await playAgentResponse(agent.id, agent.name, response.text, response.voice)

        // ë‹¤ìŒ í„´ ì „ ì ì‹œ ëŒ€ê¸°
        await new Promise(resolve => setTimeout(resolve, 200))
      }
    }

    // í† ë¡  ì™„ë£Œ - ì‚¬ìš©ìì—ê²Œ ë§ˆì´í¬ ë„˜ê¹€
    setCurrentSpeaker(null)
    setSpeakingStatus("idle")
    turnLockRef.current = false
    isProcessingResponseRef.current = false

    console.log("[MeetingVoice] âœ… Discussion complete, your turn!")
  }, [meetingTopic, currentUserName, agentParticipants, getAgentResponse, playAgentResponse, onMessage, onTranscript, sharedDocuments])

  // ì—°ê²° ì‚¬ìš´ë“œ
  const playConnectionSound = useCallback(() => {
    try {
      const ctx = new AudioContext()
      const osc = ctx.createOscillator()
      const gain = ctx.createGain()
      osc.connect(gain)
      gain.connect(ctx.destination)
      osc.type = "sine"
      osc.frequency.setValueAtTime(523.25, ctx.currentTime)
      osc.frequency.setValueAtTime(659.25, ctx.currentTime + 0.15)
      gain.gain.setValueAtTime(0.2, ctx.currentTime)
      gain.gain.exponentialRampToValueAtTime(0.01, ctx.currentTime + 0.4)
      osc.start(ctx.currentTime)
      osc.stop(ctx.currentTime + 0.4)
      setTimeout(() => ctx.close(), 500)
    } catch (e) {
      console.log("[MeetingVoice] Sound skipped")
    }
  }, [])

  // ì˜¤ë””ì˜¤ ì²­í¬ ì¬ìƒ
  const playAudioChunk = useCallback((base64Audio: string) => {
    if (!audioContextRef.current || isSpeakerMuted) return

    try {
      const binaryString = atob(base64Audio)
      const bytes = new Uint8Array(binaryString.length)
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i)
      }

      const pcm16 = new Int16Array(bytes.buffer)
      const float32 = new Float32Array(pcm16.length)
      for (let i = 0; i < pcm16.length; i++) {
        float32[i] = pcm16[i] / 32768.0
      }

      audioQueueRef.current.push(float32)

      if (!isPlayingRef.current) {
        playNextChunk()
      }
    } catch (e) {
      console.error("[MeetingVoice] Audio decode error:", e)
    }
  }, [isSpeakerMuted])

  const playNextChunk = useCallback(() => {
    if (!audioContextRef.current || audioQueueRef.current.length === 0) {
      isPlayingRef.current = false
      return
    }

    isPlayingRef.current = true
    const chunk = audioQueueRef.current.shift()!

    const buffer = audioContextRef.current.createBuffer(1, chunk.length, 24000)
    buffer.getChannelData(0).set(chunk)

    const source = audioContextRef.current.createBufferSource()
    source.buffer = buffer
    source.connect(audioContextRef.current.destination)
    source.onended = () => playNextChunk()
    source.start()
  }, [])

  // WebSocket ì—°ê²°
  const connect = useCallback(async () => {
    if (status === "connecting" || status === "connected") return

    setStatus("connecting")
    setMessages([])
    setCurrentTranscript("")

    try {
      // í† í° ë°œê¸‰
      const tokenRes = await fetch("/api/grok-voice/token", { method: "POST" })
      if (!tokenRes.ok) throw new Error("Failed to get token")

      const tokenData = await tokenRes.json()
      const token = tokenData.client_secret

      if (!token) {
        console.error("[MeetingVoice] No token received")
        setStatus("error")
        return
      }

      // AudioContext ìƒì„±
      audioContextRef.current = new AudioContext({ sampleRate: 24000 })

      // ğŸ”¥ íšŒì˜ ì „ìš© ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ê°œì¸ ì±„íŒ… ë©”ëª¨ë¦¬ ì œì™¸!)
      agentContextsRef.current.clear()
      const otherAgentNames = agentParticipants.map(a => a.name)

      for (const agent of agentParticipants) {
        try {
          console.log("[MeetingVoice] Loading MEETING context for:", agent.name)
          // ğŸ”¥ íšŒì˜ ì „ìš© API ì‚¬ìš© (ê°œì¸ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì œì™¸)
          const otherParticipants = otherAgentNames.filter(n => n !== agent.name)
          const contextRes = await fetch(
            `/api/grok-voice/meeting-context?agentId=${agent.id}&topic=${encodeURIComponent(meetingTopic)}&participants=${encodeURIComponent(otherParticipants.join(','))}`
          )

          if (contextRes.ok) {
            const contextData = await contextRes.json()
            const voice = contextData.voiceSettings?.voice || getVoiceForAgent(agent)

            // ğŸ”¥ íšŒì˜ìš© ì»¨í…ìŠ¤íŠ¸ ì €ì¥ (ê°œì¸ ë©”ëª¨ë¦¬ ì—†ìŒ)
            agentContextsRef.current.set(agent.id, {
              prompt: contextData.systemPrompt || '',
              voice: voice,
              chatHistory: [],  // íšŒì˜ ì¤‘ ëŒ€í™” íˆìŠ¤í† ë¦¬
            })

            console.log("[MeetingVoice] âœ… Meeting context loaded for:", agent.name, {
              voice,
              isMeetingContext: contextData.isMeetingContext,
              otherParticipants: contextData.otherParticipants,
            })
          } else {
            // ê¸°ë³¸ íšŒì˜ ì»¨í…ìŠ¤íŠ¸
            agentContextsRef.current.set(agent.id, {
              prompt: `ë‹¹ì‹ ì€ ${agent.name}ì…ë‹ˆë‹¤. "${meetingTopic}" íšŒì˜ì— ì°¸ì—¬ ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì°¸ì—¬ì: ${otherAgentNames.filter(n => n !== agent.name).join(', ')}`,
              voice: getVoiceForAgent(agent),
              chatHistory: [],
            })
            console.warn("[MeetingVoice] âš ï¸ Using default meeting context for:", agent.name)
          }
        } catch (err) {
          console.error("[MeetingVoice] âŒ Meeting context load error for:", agent.name, err)
          agentContextsRef.current.set(agent.id, {
            prompt: `ë‹¹ì‹ ì€ ${agent.name}ì…ë‹ˆë‹¤. íšŒì˜ ì°¸ì—¬ ì¤‘.`,
            voice: getVoiceForAgent(agent),
            chatHistory: [],
          })
        }
      }

      // ğŸ”¥ STT ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (AI ì‘ë‹µ ìƒì„± ì•ˆ í•¨)
      const systemPrompt = `ë‹¹ì‹ ì€ ìŒì„± ì¸ì‹(STT) ì „ìš©ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ë§í•˜ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ë§Œ í•´ì£¼ì„¸ìš”.
ì ˆëŒ€ë¡œ AI ì‘ë‹µì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
íšŒì˜ ì£¼ì œ: ${meetingTopic}`

      // WebSocket ì—°ê²°
      const ws = new WebSocket(
        "wss://api.x.ai/v1/realtime?model=grok-3-fast-realtime",
        ["realtime", `openai-insecure-api-key.${token}`, "openai-beta.realtime-v1"]
      )
      wsRef.current = ws

      ws.onopen = () => {
        console.log("[MeetingVoice] Connected (STT-only mode)")

        // ğŸ”¥ STT ì „ìš© ì„¸ì…˜ (AI ì‘ë‹µ ìƒì„± ë¹„í™œì„±í™”)
        ws.send(JSON.stringify({
          type: "session.update",
          session: {
            modalities: ["text"],  // ğŸ”¥ audio ì œê±° - STTë§Œ ì‚¬ìš©
            instructions: systemPrompt,
            voice: "sol",  // ì‚¬ìš© ì•ˆ í•¨
            input_audio_format: "pcm16",
            output_audio_format: "pcm16",
            input_audio_transcription: { model: "whisper-1" },
            turn_detection: {
              type: "server_vad",
              threshold: 0.5,
              prefix_padding_ms: 200,
              silence_duration_ms: 600,
            },
          },
        }))

        setStatus("connected")
        playConnectionSound()

        // ë§ˆì´í¬ ì‹œì‘
        setTimeout(() => startMicrophone(), 500)
      }

      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data)
          handleServerEvent(data)
        } catch (e) {
          console.error("[MeetingVoice] Parse error:", e)
        }
      }

      ws.onerror = (error) => {
        console.error("[MeetingVoice] Error:", error)
        setStatus("error")
      }

      ws.onclose = () => {
        console.log("[MeetingVoice] Disconnected")
        setStatus("disconnected")
        stopMicrophone()
      }

    } catch (error) {
      console.error("[MeetingVoice] Connection error:", error)
      setStatus("error")
    }
  }, [status, meetingTopic, currentUserName, agentParticipants, playConnectionSound])

  // ì„œë²„ ì´ë²¤íŠ¸ ì²˜ë¦¬
  const handleServerEvent = useCallback((data: any) => {
    switch (data.type) {
      case "session.created":
        console.log("[MeetingVoice] Session created")
        break

      // ì‚¬ìš©ì ìŒì„± ê°ì§€ ì‹œì‘
      case "input_audio_buffer.speech_started":
        if (!turnLockRef.current) {
          setCurrentSpeaker(currentUserId)
          setSpeakingStatus("speaking")
          turnLockRef.current = true
          console.log("[MeetingVoice] ğŸ¤ User speaking...")
        }
        break

      // ì‚¬ìš©ì ìŒì„± ê°ì§€ ì¢…ë£Œ
      case "input_audio_buffer.speech_stopped":
        setSpeakingStatus("waiting")
        console.log("[MeetingVoice] ğŸ”‡ User stopped speaking, waiting for transcription...")
        break

      // ì‚¬ìš©ì ë°œì–¸ íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ì™„ë£Œ
      case "conversation.item.input_audio_transcription.completed":
        const userText = data.transcript || ""
        setCurrentTranscript("")

        if (!userText.trim()) {
          // ë¹ˆ í…ìŠ¤íŠ¸ë©´ ë¬´ì‹œ
          turnLockRef.current = false
          setSpeakingStatus("idle")
          break
        }

        // ë©”ì‹œì§€ ì €ì¥
        const userMsg: VoiceMessage = {
          id: `user-${Date.now()}`,
          participantId: currentUserId,
          participantName: currentUserName,
          participantType: "user",
          text: userText,
          timestamp: new Date(),
          isComplete: true,
        }
        setMessages(prev => [...prev, userMsg])
        onMessage?.(userMsg)
        onTranscript?.(userText, currentUserId)

        console.log("[MeetingVoice] ğŸ“ User said:", userText)

        // ğŸ”¥ ê° ì—ì´ì „íŠ¸ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì‘ë‹µí•˜ë„ë¡ íŠ¸ë¦¬ê±°
        processAgentResponses(userText)
        break

      // ğŸ”¥ AI ì‘ë‹µì€ ì‚¬ìš© ì•ˆ í•¨ (STT ì „ìš©)
      case "response.output_audio.delta":
      case "response.output_audio_transcript.delta":
      case "response.output_audio_transcript.done":
      case "response.done":
        // STT ì „ìš© ëª¨ë“œì—ì„œëŠ” ë¬´ì‹œ
        console.log("[MeetingVoice] Ignoring AI response (STT-only mode)")
        break

      case "error":
        console.error("[MeetingVoice] Server error:", data.error)
        break
    }
  }, [currentUserId, currentUserName, onMessage, onTranscript, processAgentResponses])

  // ë§ˆì´í¬ ì‹œì‘
  const startMicrophone = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        },
      })

      mediaStreamRef.current = stream

      if (!audioContextRef.current) {
        audioContextRef.current = new AudioContext({ sampleRate: 24000 })
      }

      const source = audioContextRef.current.createMediaStreamSource(stream)
      const processor = audioContextRef.current.createScriptProcessor(4096, 1, 1)
      processorRef.current = processor

      processor.onaudioprocess = (e) => {
        // ìŒì†Œê±° ìƒíƒœê±°ë‚˜ ë‹¤ë¥¸ ì‚¬ëŒì´ ë§í•˜ê³  ìˆìœ¼ë©´ ì „ì†¡ ì•ˆí•¨
        if (isMuted || !wsRef.current || wsRef.current.readyState !== WebSocket.OPEN) return
        if (currentSpeaker && currentSpeaker !== currentUserId && currentSpeaker !== null) return

        const inputData = e.inputBuffer.getChannelData(0)
        const pcm16 = new Int16Array(inputData.length)
        for (let i = 0; i < inputData.length; i++) {
          const s = Math.max(-1, Math.min(1, inputData[i]))
          pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF
        }

        const bytes = new Uint8Array(pcm16.buffer)
        let binary = ""
        for (let i = 0; i < bytes.byteLength; i++) {
          binary += String.fromCharCode(bytes[i])
        }
        const base64 = btoa(binary)

        wsRef.current.send(JSON.stringify({
          type: "input_audio_buffer.append",
          audio: base64,
        }))
      }

      source.connect(processor)
      processor.connect(audioContextRef.current.destination)

      console.log("[MeetingVoice] Microphone started")
    } catch (error) {
      console.error("[MeetingVoice] Microphone error:", error)
    }
  }, [isMuted, currentSpeaker, currentUserId])

  // ë§ˆì´í¬ ì¤‘ì§€
  const stopMicrophone = useCallback(() => {
    if (processorRef.current) {
      processorRef.current.disconnect()
      processorRef.current = null
    }

    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop())
      mediaStreamRef.current = null
    }

    console.log("[MeetingVoice] Microphone stopped")
  }, [])

  // ì—°ê²° í•´ì œ
  const disconnect = useCallback(() => {
    stopMicrophone()

    if (wsRef.current) {
      wsRef.current.close()
      wsRef.current = null
    }

    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }

    audioQueueRef.current = []
    isPlayingRef.current = false
    turnLockRef.current = false
    isProcessingResponseRef.current = false
    meetingHistoryRef.current = []  // ğŸ”¥ íšŒì˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    agentContextsRef.current.clear()  // ğŸ”¥ ì—ì´ì „íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”

    setStatus("disconnected")
    setCurrentSpeaker(null)
    setSpeakingStatus("idle")
    setMessages([])  // ğŸ”¥ ë©”ì‹œì§€ ì´ˆê¸°í™”
  }, [stopMicrophone])

  // ì†ë“¤ê¸° (ë°œì–¸ê¶Œ ìš”ì²­)
  const raiseHand = useCallback(() => {
    if (currentSpeaker && currentSpeaker !== currentUserId) {
      setHandRaised(true)
      setTurnQueue(prev => [...prev, currentUserId])
      console.log("[MeetingVoice] âœ‹ Hand raised")
    }
  }, [currentSpeaker, currentUserId])

  // ğŸ“ ë¦¬ì‚¬ì´ì¦ˆ í•¸ë“¤ëŸ¬
  const handleResizeStart = useCallback((e: React.MouseEvent) => {
    e.preventDefault()
    isResizingRef.current = true
    resizeStartXRef.current = e.clientX
    resizeStartWidthRef.current = panelWidth
    document.body.style.cursor = 'ew-resize'
    document.body.style.userSelect = 'none'
  }, [panelWidth])

  const handleResizeMove = useCallback((e: MouseEvent) => {
    if (!isResizingRef.current) return
    const deltaX = e.clientX - resizeStartXRef.current
    const newWidth = Math.max(200, Math.min(600, resizeStartWidthRef.current + deltaX))
    setPanelWidth(newWidth)
  }, [])

  const handleResizeEnd = useCallback(() => {
    isResizingRef.current = false
    document.body.style.cursor = ''
    document.body.style.userSelect = ''
  }, [])

  // ë¦¬ì‚¬ì´ì¦ˆ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ
  useEffect(() => {
    window.addEventListener('mousemove', handleResizeMove)
    window.addEventListener('mouseup', handleResizeEnd)
    return () => {
      window.removeEventListener('mousemove', handleResizeMove)
      window.removeEventListener('mouseup', handleResizeEnd)
    }
  }, [handleResizeMove, handleResizeEnd])

  // ğŸ”­ ë·°íŒŒì¸ë” ìº¡ì²˜ í•¸ë“¤ëŸ¬
  const handleViewfinderCapture = useCallback((result: ViewfinderCaptureResult) => {
    console.log("[MeetingVoice] ğŸ”­ Viewfinder captured:", result.bounds)
    if (onShareFile) {
      onShareFile({
        dataUrl: result.imageDataUrl,
        name: `screen-capture-${Date.now()}.jpg`,
        type: 'image/jpeg'
      })
    }
    // ìº¡ì²˜ í›„ ë·°íŒŒì¸ë” ë‹«ê¸°
    setShowViewfinder(false)
  }, [onShareFile])

  // ğŸ”­ ë·°íŒŒì¸ë” AI ê³µìœ  í•¸ë“¤ëŸ¬
  const handleViewfinderShare = useCallback((imageDataUrl: string, timestamp: number) => {
    console.log("[MeetingVoice] ğŸ”­ Viewfinder sharing to AI:", timestamp)
    if (onShareFile) {
      onShareFile({
        dataUrl: imageDataUrl,
        name: `screen-share-${timestamp}.jpg`,
        type: 'image/jpeg'
      })
    }
  }, [onShareFile])

  // ğŸ“ íŒŒì¼ ì„ íƒ í•¸ë“¤ëŸ¬
  const handleFileSelect = useCallback((e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0]
    if (file && onShareFile) {
      console.log("[MeetingVoice] ğŸ“ File selected:", file.name)
      onShareFile(file)
    }
    // ì…ë ¥ ì´ˆê¸°í™” (ê°™ì€ íŒŒì¼ ë‹¤ì‹œ ì„ íƒ ê°€ëŠ¥)
    if (fileInputRef.current) {
      fileInputRef.current.value = ''
    }
  }, [onShareFile])

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      disconnect()
    }
  }, [disconnect])

  // UI í—¬í¼
  const getParticipantById = (id: string) => participants.find(p => p.id === id)
  const getSpeakerInfo = () => {
    if (!currentSpeaker) return null
    // ì—ì´ì „íŠ¸ IDë¡œ ì§ì ‘ ì°¾ê¸°
    const agent = agentParticipants.find(a => a.id === currentSpeaker)
    if (agent) return agent
    // ì‚¬ìš©ìì¸ ê²½ìš°
    if (currentSpeaker === currentUserId) {
      return { id: currentUserId, name: currentUserName, type: "user" as const }
    }
    return null
  }

  const speakerInfo = getSpeakerInfo()

  // í˜„ì¬ ì‘ë‹µ ì¤‘ì¸ ì—ì´ì „íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
  const getCurrentAgentVoice = () => {
    if (!currentSpeaker) return null
    const ctx = agentContextsRef.current.get(currentSpeaker)
    return ctx?.voice || null
  }

  // ğŸ”¥ ì»´íŒ©íŠ¸í•œ í”Œë¡œíŒ… ìŒì„± íŒ¨ë„ (ì±„íŒ…ì°½ ê°€ë¦¬ì§€ ì•ŠìŒ)
  return (
    <>
      {/* ğŸ”­ AIViewfinder ì»´í¬ë„ŒíŠ¸ */}
      {showViewfinder && (
        <AIViewfinder
          isActive={showViewfinder}
          onClose={() => setShowViewfinder(false)}
          onCapture={handleViewfinderCapture}
          onShareToAI={handleViewfinderShare}
          aiContextEnabled={true}
          mode="manual"
          initialBounds={{ x: 100, y: 100, width: 400, height: 300 }}
        />
      )}

      {/* ìˆ¨ê²¨ì§„ íŒŒì¼ ì…ë ¥ */}
      <input
        ref={fileInputRef}
        type="file"
        accept="image/*,application/pdf,.doc,.docx,.xls,.xlsx,.ppt,.pptx"
        onChange={handleFileSelect}
        className="hidden"
      />

      <div
        ref={panelRef}
        className="bg-zinc-900/95 backdrop-blur-sm rounded-xl border border-zinc-700 shadow-xl p-3 relative group"
        style={{ width: panelWidth }}
      >
        {/* ğŸ“ ì¢Œì¸¡ ë¦¬ì‚¬ì´ì¦ˆ í•¸ë“¤ */}
        <div
          onMouseDown={handleResizeStart}
          className="absolute left-0 top-0 bottom-0 w-2 cursor-ew-resize flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity hover:bg-zinc-600/30 rounded-l-xl"
          title="ë“œë˜ê·¸í•˜ì—¬ í¬ê¸° ì¡°ì ˆ"
        >
          <GripVertical className="w-3 h-3 text-zinc-500" />
        </div>

        {/* ğŸ“ ìš°ì¸¡ ë¦¬ì‚¬ì´ì¦ˆ í•¸ë“¤ */}
        <div
          onMouseDown={handleResizeStart}
          className="absolute right-0 top-0 bottom-0 w-2 cursor-ew-resize flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity hover:bg-zinc-600/30 rounded-r-xl"
          title="ë“œë˜ê·¸í•˜ì—¬ í¬ê¸° ì¡°ì ˆ"
        >
          <GripVertical className="w-3 h-3 text-zinc-500" />
        </div>

        {/* ğŸ”­ ê³µìœ ëœ ìë£Œ í‘œì‹œ (í´ë”© ê°€ëŠ¥) */}
        {sharedDocuments.length > 0 && (
          <div className="mb-2">
            {/* í´ë”© í—¤ë” */}
            <button
              onClick={() => setIsDocsCollapsed(!isDocsCollapsed)}
              className="w-full flex items-center justify-between px-2 py-1 bg-zinc-800/50 rounded-lg hover:bg-zinc-800 transition-colors mb-1"
            >
              <div className="flex items-center gap-1.5">
                <ImageIcon className="w-3 h-3 text-purple-400" />
                <span className="text-xs text-zinc-400">ê³µìœ  ìë£Œ</span>
                <span className="text-[10px] text-zinc-500 bg-zinc-700 px-1.5 rounded-full">
                  {sharedDocuments.length}
                </span>
              </div>
              {isDocsCollapsed ? (
                <ChevronDown className="w-3.5 h-3.5 text-zinc-500" />
              ) : (
                <ChevronUp className="w-3.5 h-3.5 text-zinc-500" />
              )}
            </button>

            {/* í´ë”© ì»¨í…ì¸  */}
            <AnimatePresence>
              {!isDocsCollapsed && (
                <motion.div
                  initial={{ height: 0, opacity: 0 }}
                  animate={{ height: "auto", opacity: 1 }}
                  exit={{ height: 0, opacity: 0 }}
                  transition={{ duration: 0.2 }}
                  className="overflow-hidden"
                >
                  <div className="flex items-center gap-1 overflow-x-auto pb-1 px-1">
                    {sharedDocuments.map((doc) => (
                      <div
                        key={doc.id}
                        className="flex-shrink-0 flex items-center gap-1 px-2 py-1 bg-zinc-800 rounded-md text-xs hover:bg-zinc-700 transition-colors cursor-pointer"
                        title={doc.analysis || doc.name}
                      >
                        <ImageIcon className="w-3 h-3 text-purple-400" />
                        <span className="text-zinc-400 truncate max-w-[80px]">{doc.name}</span>
                        {doc.analysis && <span className="text-green-400">âœ“</span>}
                      </div>
                    ))}
                  </div>
                </motion.div>
              )}
            </AnimatePresence>
          </div>
        )}

        {/* ìƒíƒœ + ì»¨íŠ¸ë¡¤ í•œ ì¤„ */}
        <div className="flex items-center justify-between gap-2">
          {/* í˜„ì¬ ìƒíƒœ */}
          <div className="flex items-center gap-2 flex-1 min-w-0">
          {status === "connected" ? (
            <AnimatePresence mode="wait">
              {speakingStatus === "speaking" && (
                <motion.div className="flex items-center gap-2" initial={{ opacity: 0 }} animate={{ opacity: 1 }}>
                  <div className="flex gap-0.5">
                    {[...Array(3)].map((_, i) => (
                      <motion.div
                        key={i}
                        className="w-0.5 bg-emerald-500 rounded-full"
                        animate={{ height: [4, 12, 4] }}
                        transition={{ duration: 0.4, repeat: Infinity, delay: i * 0.1 }}
                      />
                    ))}
                  </div>
                  <span className="text-xs text-emerald-400 truncate">ë§í•˜ëŠ” ì¤‘...</span>
                </motion.div>
              )}
              {speakingStatus === "listening" && speakerInfo && (
                <motion.div className="flex items-center gap-2" initial={{ opacity: 0 }} animate={{ opacity: 1 }}>
                  <div className="flex gap-0.5">
                    {[...Array(3)].map((_, i) => (
                      <motion.div
                        key={i}
                        className="w-0.5 bg-purple-500 rounded-full"
                        animate={{ height: [4, 10, 4] }}
                        transition={{ duration: 0.3, repeat: Infinity, delay: i * 0.1 }}
                      />
                    ))}
                  </div>
                  <span className="text-xs text-purple-400 truncate">{speakerInfo.name}</span>
                </motion.div>
              )}
              {speakingStatus === "waiting" && (
                <motion.div className="flex items-center gap-2" initial={{ opacity: 0 }} animate={{ opacity: 1 }}>
                  <Loader2 className="w-3 h-3 text-cyan-500 animate-spin" />
                  <span className="text-xs text-cyan-400">ì²˜ë¦¬ì¤‘</span>
                </motion.div>
              )}
              {speakingStatus === "idle" && (
                <span className="text-xs text-zinc-500">{isMuted ? "ğŸ”‡" : "ğŸ¤ ëŒ€ê¸°"}</span>
              )}
            </AnimatePresence>
          ) : status === "connecting" ? (
            <div className="flex items-center gap-2">
              <Loader2 className="w-3 h-3 text-cyan-500 animate-spin" />
              <span className="text-xs text-cyan-400">ì—°ê²°ì¤‘</span>
            </div>
          ) : (
            <span className="text-xs text-zinc-500">ìŒì„± ëŒ€ê¸°</span>
          )}
        </div>

        {/* ì»¨íŠ¸ë¡¤ ë²„íŠ¼ */}
        <div className="flex items-center gap-1">
          {status === "connected" && (
            <>
              {/* ğŸŒ ë¸Œë¼ìš°ì € ë²„íŠ¼ */}
              {onOpenBrowser && (
                <button
                  onClick={onOpenBrowser}
                  className="w-7 h-7 rounded-full flex items-center justify-center transition-all bg-zinc-800 text-white hover:bg-zinc-700 hover:bg-blue-500/20 hover:text-blue-400"
                  title="ì›¹ ë¸Œë¼ìš°ì € ì—´ê¸°"
                >
                  <Globe className="w-3.5 h-3.5" />
                </button>
              )}
              {/* ğŸ”­ ë·°íŒŒì¸ë” ë²„íŠ¼ */}
              <button
                onClick={() => setShowViewfinder(!showViewfinder)}
                className={`w-7 h-7 rounded-full flex items-center justify-center transition-all ${
                  showViewfinder ? "bg-purple-500/20 text-purple-400" : "bg-zinc-800 text-white hover:bg-zinc-700"
                }`}
                title="í™”ë©´ ìº¡ì²˜ (ë·°íŒŒì¸ë”)"
              >
                <Eye className="w-3.5 h-3.5" />
              </button>
              {/* ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë²„íŠ¼ */}
              <button
                onClick={() => fileInputRef.current?.click()}
                className="w-7 h-7 rounded-full flex items-center justify-center transition-all bg-zinc-800 text-white hover:bg-zinc-700"
                title="íŒŒì¼ ê³µìœ "
              >
                <Upload className="w-3.5 h-3.5" />
              </button>
              {/* ğŸ”‡ ë§ˆì´í¬ ìŒì†Œê±° */}
              <button
                onClick={() => setIsMuted(!isMuted)}
                className={`w-7 h-7 rounded-full flex items-center justify-center transition-all ${
                  isMuted ? "bg-red-500/20 text-red-400" : "bg-zinc-800 text-white hover:bg-zinc-700"
                }`}
              >
                {isMuted ? <MicOff className="w-3.5 h-3.5" /> : <Mic className="w-3.5 h-3.5" />}
              </button>
              {/* ğŸ”Š ìŠ¤í”¼ì»¤ ìŒì†Œê±° */}
              <button
                onClick={() => setIsSpeakerMuted(!isSpeakerMuted)}
                className={`w-7 h-7 rounded-full flex items-center justify-center transition-all ${
                  isSpeakerMuted ? "bg-amber-500/20 text-amber-400" : "bg-zinc-800 text-white hover:bg-zinc-700"
                }`}
              >
                {isSpeakerMuted ? <VolumeX className="w-3.5 h-3.5" /> : <Volume2 className="w-3.5 h-3.5" />}
              </button>
            </>
          )}

          {/* ì—°ê²°/ì¢…ë£Œ */}
          {status === "disconnected" || status === "error" ? (
            <button
              onClick={connect}
              className="w-8 h-8 rounded-full bg-emerald-500 hover:bg-emerald-400 text-white flex items-center justify-center transition-all"
            >
              <Phone className="w-4 h-4" />
            </button>
          ) : status === "connecting" ? (
            <button disabled className="w-8 h-8 rounded-full bg-cyan-500 text-white flex items-center justify-center">
              <Loader2 className="w-4 h-4 animate-spin" />
            </button>
          ) : (
            <button
              onClick={disconnect}
              className="w-8 h-8 rounded-full bg-red-500 hover:bg-red-400 text-white flex items-center justify-center transition-all"
            >
              <PhoneOff className="w-4 h-4" />
            </button>
          )}
        </div>
      </div>
      </div>
    </>
  )
}
