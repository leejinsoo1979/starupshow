/**
 * Super Agent Chat - Tool Calling ì§€ì› ì±„íŒ… ì‹œìŠ¤í…œ
 * Cursor/Claude Codeê¸‰ ì—ì´ì „íŠ¸ ê¸°ëŠ¥
 */

import { ChatOpenAI } from '@langchain/openai'
import { ChatOllama } from '@langchain/ollama'
import { HumanMessage, SystemMessage, AIMessage, ToolMessage } from '@langchain/core/messages'
import { getSuperAgentTools, ToolAction } from './super-agent-tools'
import { getDefaultModel, LLMProvider } from '@/lib/llm/client'
import {
  buildDynamicAgentSystemPrompt,
  AGENT_ROLE_PROMPTS,
} from '@/lib/agent/shared-prompts'

// ============================================
// íƒ€ì… ì •ì˜
// ============================================
export interface SuperAgentMessage {
  role: 'user' | 'assistant' | 'system' | 'tool'
  content: string
  toolCalls?: ToolCallInfo[]
  toolCallId?: string
}

export interface ToolCallInfo {
  id: string
  name: string
  arguments: Record<string, unknown>
}

export interface SuperAgentResponse {
  message: string
  actions: ToolAction[]  // í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤í–‰í•  ì•¡ì…˜ë“¤
  toolsUsed: string[]
  thinking?: string
}

interface AgentConfig {
  id: string
  name: string
  description?: string
  capabilities?: string[]
  llm_provider?: string | null
  model?: string | null
  temperature?: number | null
  system_prompt?: string | null
  identity?: any
  apiKey?: string | null
}

interface ChatContext {
  projectPath?: string | null
  userName?: string
  userRole?: string
  workContext?: string
  files?: Array<{ path: string; content?: string }>
}

// ============================================
// LLM ìƒì„±
// ============================================
function createLLM(provider: LLMProvider, model: string, apiKey?: string, temperature = 0.7) {
  switch (provider) {
    case 'openai':
      return new ChatOpenAI({
        model,
        temperature,
        apiKey: apiKey || process.env.OPENAI_API_KEY,
      })

    case 'grok':
      return new ChatOpenAI({
        model,
        temperature,
        apiKey: apiKey || process.env.XAI_API_KEY,
        configuration: {
          baseURL: 'https://api.x.ai/v1',
        },
      })

    case 'gemini':
      return new ChatOpenAI({
        model,
        temperature,
        apiKey: apiKey || process.env.GOOGLE_API_KEY,
        configuration: {
          baseURL: 'https://generativelanguage.googleapis.com/v1beta/openai/',
        },
      })

    case 'qwen':
      return new ChatOpenAI({
        model,
        temperature,
        apiKey: apiKey || process.env.DASHSCOPE_API_KEY,
        configuration: {
          baseURL: 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1',
        },
      })

    case 'ollama':
      return new ChatOllama({
        model,
        temperature,
        baseUrl: 'http://localhost:11434',
      })

    default:
      return new ChatOllama({
        model: 'qwen2.5:3b',
        temperature: 0.7,
      })
  }
}

// ============================================
// ì—­í•  ì¶”ì¶œ
// ============================================
function getAgentRole(capabilities: string[]): string {
  if (capabilities.includes('development') || capabilities.includes('coding')) return 'developer'
  if (capabilities.includes('design') || capabilities.includes('ui')) return 'designer'
  if (capabilities.includes('marketing') || capabilities.includes('growth')) return 'marketer'
  if (capabilities.includes('analytics') || capabilities.includes('data')) return 'analyst'
  if (capabilities.includes('management') || capabilities.includes('planning')) return 'pm'
  return 'default'
}

// ============================================
// ìŠˆí¼ ì—ì´ì „íŠ¸ ì±„íŒ… ì‘ë‹µ ìƒì„± (Tool Calling ì§€ì›)
// ============================================
export async function generateSuperAgentResponse(
  agent: AgentConfig,
  userMessage: string,
  chatHistory: SuperAgentMessage[] = [],
  context?: ChatContext
): Promise<SuperAgentResponse> {
  // LLM ì„¤ì •
  const provider = (agent.llm_provider || 'grok') as LLMProvider
  const model = agent.model || getDefaultModel(provider)
  const temperature = agent.temperature ?? 0.7

  console.log(`[SuperAgent] ${agent.name} using ${provider}/${model} with tool calling`)

  // LLM ìƒì„±
  const llm = createLLM(provider, model, agent.apiKey || undefined, temperature)

  // ë„êµ¬ ë°”ì¸ë”©
  const tools = getSuperAgentTools()
  const llmWithTools = llm.bindTools(tools)

  // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
  const role = getAgentRole(agent.capabilities || [])
  const basePersonality = agent.system_prompt || AGENT_ROLE_PROMPTS[role] || AGENT_ROLE_PROMPTS['default']

  // ì •ì²´ì„± ì •ë³´
  let identityStr = ''
  if (agent.identity) {
    const id = agent.identity
    const parts: string[] = ['## ğŸ§  ë‹¹ì‹ ì˜ ì •ì²´ì„±ê³¼ ì„±ê²©']
    if (id.self_summary) parts.push(`\n### ë‚˜ëŠ” ëˆ„êµ¬ì¸ê°€\n${id.self_summary}`)
    if (id.core_values?.length) parts.push(`\n### í•µì‹¬ ê°€ì¹˜\n${id.core_values.map((v: string) => `- ${v}`).join('\n')}`)
    if (id.personality_traits?.length) parts.push(`\n### ì„±ê²© íŠ¹ì„±\n${id.personality_traits.map((t: string) => `- ${t}`).join('\n')}`)
    if (id.communication_style) parts.push(`\n### ì†Œí†µ ìŠ¤íƒ€ì¼\n${id.communication_style}`)
    identityStr = parts.join('\n')
  }

  const coreSystemPrompt = buildDynamicAgentSystemPrompt(
    agent.name,
    basePersonality,
    identityStr,
    '',
    false
  )

  // í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸
  const projectContext = context?.projectPath
    ? `\n## ğŸ“ í˜„ì¬ í”„ë¡œì íŠ¸\n- ê²½ë¡œ: ${context.projectPath}\n`
    : ''

  // ì‚¬ìš©ì ì •ë³´
  const userInfo = context?.userName
    ? `\n## ğŸ‘¤ ëŒ€í™” ìƒëŒ€\n- ì´ë¦„: ${context.userName}${context.userRole ? `\n- ì§ìœ„: ${context.userRole}` : ''}\n`
    : ''

  // ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸
  const workContextStr = context?.workContext
    ? `\n## ğŸ“‹ ì—…ë¬´ ë§¥ë½\n${context.workContext}\n`
    : ''

  // íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
  const filesContext = context?.files?.length
    ? `\n## ğŸ“„ ë¡œë“œëœ íŒŒì¼ë“¤\n${context.files.map(f => `- ${f.path}`).join('\n')}\n`
    : ''

  const systemPrompt = `${coreSystemPrompt}

${projectContext}
${userInfo}
${workContextStr}
${filesContext}

## ğŸ› ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬
ë‹¹ì‹ ì€ ë‹¤ìŒ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **create_project** - ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
2. **read_file** - íŒŒì¼ ì½ê¸°
3. **write_file** - íŒŒì¼ ìƒì„±/ë®ì–´ì“°ê¸° â­ ì½”ë“œ ì‘ì„± ì‹œ í•„ìˆ˜
4. **edit_file** - íŒŒì¼ ë¶€ë¶„ ìˆ˜ì •
5. **search_files** - íŒŒì¼/ì½”ë“œ ê²€ìƒ‰
6. **get_file_structure** - í´ë” êµ¬ì¡° ì¡°íšŒ
7. **run_terminal** - í„°ë¯¸ë„ ëª…ë ¹ ì‹¤í–‰ (npm, git ë“±)
8. **web_search** - ì›¹ ê²€ìƒ‰
9. **create_task** - íƒœìŠ¤í¬ ìƒì„±
10. **list_projects** - í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ
11. **generate_image** - AI ì´ë¯¸ì§€ ìƒì„±

## ğŸš¨ ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ - ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”!

### âŒ ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ:
- "~í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤" ê°™ì€ ì„¤ëª…ë§Œ í•˜ê¸°
- "~ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤" ê°™ì€ ê¸°íšì„œ ì‘ì„±
- ì™¸ë¶€ ë„êµ¬(draw.io, Figma ë“±) ì¶”ì²œí•˜ê¸°
- "ì–´ë–»ê²Œ í• ê¹Œìš”?" ì—­ì§ˆë¬¸í•˜ê¸°

### âœ… ë°˜ë“œì‹œ í•  ê²ƒ:
- **ì¦‰ì‹œ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤ì œë¡œ ì‘ì—… ìˆ˜í–‰**
- ì½”ë“œ ì‘ì„± ìš”ì²­ â†’ write_fileë¡œ íŒŒì¼ ìƒì„±
- ê°œë°œ ìš”ì²­ â†’ ì½”ë“œ ì‘ì„± + íŒŒì¼ ìƒì„±
- íŒŒì¼ ìˆ˜ì • ìš”ì²­ â†’ edit_fileë¡œ ìˆ˜ì •
- ì„¤ì¹˜ ìš”ì²­ â†’ run_terminalë¡œ npm install ì‹¤í–‰

### ì˜ˆì‹œ:
- ì‚¬ìš©ì: "ìë™ì°¨ ì»¨í”¼ê·œë ˆì´í„° ê°œë°œí•´ì¤˜"
- âŒ ë‚˜ìœ ì‘ë‹µ: "ë„¤, ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Three.jsë¥¼ ì‚¬ìš©í•˜ë©´..."
- âœ… ì¢‹ì€ ì‘ë‹µ: write_file ë„êµ¬ë¡œ CarConfigurator.tsx íŒŒì¼ ìƒì„±

- ì‚¬ìš©ì: "í”Œë¡œìš°ì°¨íŠ¸ ê·¸ë ¤ì¤˜"
- âŒ ë‚˜ìœ ì‘ë‹µ: "draw.ioë¥¼ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤"
- âœ… ì¢‹ì€ ì‘ë‹µ: write_fileë¡œ í”Œë¡œìš°ì°¨íŠ¸ ì»´í¬ë„ŒíŠ¸ ìƒì„± ë˜ëŠ” ì§ì ‘ ë…¸ë“œ ìƒì„±

ë‹¹ì‹ ì€ **ì½”ë”© ì—ì´ì „íŠ¸**ì…ë‹ˆë‹¤. ë§ë§Œ í•˜ì§€ ë§ê³  **ì§ì ‘ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  íŒŒì¼ì„ ìƒì„±**í•˜ì„¸ìš”!
`

  // ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„±
  const messages: (SystemMessage | HumanMessage | AIMessage | ToolMessage)[] = [
    new SystemMessage(systemPrompt),
  ]

  // ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¶”ê°€
  for (const msg of chatHistory.slice(-20)) {
    if (msg.role === 'user') {
      messages.push(new HumanMessage(msg.content))
    } else if (msg.role === 'assistant') {
      messages.push(new AIMessage(msg.content))
    } else if (msg.role === 'tool' && msg.toolCallId) {
      messages.push(new ToolMessage({ content: msg.content, tool_call_id: msg.toolCallId }))
    }
  }

  // í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
  messages.push(new HumanMessage(userMessage))

  // Tool Calling ë£¨í”„
  const actions: ToolAction[] = []
  const toolsUsed: string[] = []
  let finalResponse = ''
  let iterations = 0
  const maxIterations = 5  // ë¬´í•œ ë£¨í”„ ë°©ì§€

  try {
    while (iterations < maxIterations) {
      iterations++
      console.log(`[SuperAgent] Iteration ${iterations}`)

      // LLM í˜¸ì¶œ
      const response = await llmWithTools.invoke(messages)

      // Tool Call í™•ì¸
      const toolCalls = response.tool_calls || []

      if (toolCalls.length === 0) {
        // Tool Call ì—†ìŒ - ìµœì¢… ì‘ë‹µ
        finalResponse = typeof response.content === 'string'
          ? response.content
          : JSON.stringify(response.content)
        break
      }

      // Tool Call ìˆìŒ - ë„êµ¬ ì‹¤í–‰
      messages.push(new AIMessage({
        content: response.content || '',
        tool_calls: toolCalls.map(tc => ({
          id: tc.id || `tool_${Date.now()}`,
          name: tc.name,
          args: tc.args,
        })),
      }))

      for (const toolCall of toolCalls) {
        const toolName = toolCall.name
        const toolArgs = toolCall.args || {}
        const toolId = toolCall.id || `tool_${Date.now()}`

        console.log(`[SuperAgent] Tool call: ${toolName}`, toolArgs)
        toolsUsed.push(toolName)

        // ë„êµ¬ ì°¾ê¸° ë° ì‹¤í–‰
        const tool = tools.find(t => t.name === toolName)
        if (!tool) {
          messages.push(new ToolMessage({
            content: JSON.stringify({ success: false, error: `ë„êµ¬ "${toolName}"ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.` }),
            tool_call_id: toolId,
          }))
          continue
        }

        try {
          // ë„êµ¬ ì‹¤í–‰
          const result = await tool.invoke(toolArgs)
          const parsedResult = typeof result === 'string' ? JSON.parse(result) : result

          // ì•¡ì…˜ ìˆ˜ì§‘ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‹¤í–‰í•  ê²ƒë“¤)
          if (parsedResult.action) {
            actions.push(parsedResult.action)
          }

          messages.push(new ToolMessage({
            content: result,
            tool_call_id: toolId,
          }))
        } catch (error: any) {
          messages.push(new ToolMessage({
            content: JSON.stringify({ success: false, error: error.message }),
            tool_call_id: toolId,
          }))
        }
      }
    }

    // ì‘ë‹µ ì •ë¦¬
    let cleanResponse = finalResponse
    cleanResponse = cleanResponse.replace(/<think>[\s\S]*?<\/think>\s*/g, '')
    cleanResponse = cleanResponse.replace(/<thinking>[\s\S]*?<\/thinking>\s*/g, '')

    return {
      message: cleanResponse.trim() || 'ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.',
      actions,
      toolsUsed,
    }
  } catch (error: any) {
    console.error('[SuperAgent] Error:', error)
    return {
      message: `ì£„ì†¡í•´ìš”, ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”: ${error.message}`,
      actions: [],
      toolsUsed,
    }
  }
}

// ============================================
// ì•¡ì…˜ ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬
// ============================================
export interface ActionExecutionResult {
  action: ToolAction
  success: boolean
  result?: unknown
  error?: string
}

export function formatActionResults(results: ActionExecutionResult[]): string {
  if (results.length === 0) return ''

  const lines: string[] = ['## ì‹¤í–‰ ê²°ê³¼']

  for (const r of results) {
    const status = r.success ? 'âœ…' : 'âŒ'
    const type = r.action.type

    switch (type) {
      case 'create_project':
        lines.push(`${status} í”„ë¡œì íŠ¸ ìƒì„±: ${r.action.data.name}`)
        break
      case 'write_file':
      case 'edit_file':
        lines.push(`${status} íŒŒì¼ ìˆ˜ì •: ${r.action.data.path}`)
        break
      case 'terminal_cmd':
        lines.push(`${status} ëª…ë ¹ ì‹¤í–‰: ${r.action.data.command}`)
        if (r.result) lines.push(`   ê²°ê³¼: ${String(r.result).slice(0, 200)}`)
        break
      case 'create_task':
        lines.push(`${status} íƒœìŠ¤í¬ ìƒì„±: ${r.action.data.title}`)
        break
      default:
        lines.push(`${status} ${type}: ${JSON.stringify(r.action.data).slice(0, 100)}`)
    }

    if (r.error) {
      lines.push(`   ì˜¤ë¥˜: ${r.error}`)
    }
  }

  return lines.join('\n')
}
