import { ChatOpenAI } from '@langchain/openai'
import { ChatOllama } from '@langchain/ollama'
import { HumanMessage, SystemMessage, AIMessage, ToolMessage, BaseMessage } from '@langchain/core/messages'
import { DynamicStructuredTool } from '@langchain/core/tools'
import { getToolsByNames, getAllToolNames, MCPToolName, ALL_TOOLS } from './tools'
import { loadAgentApiConnections, createAllApiTools, generateApiToolsDescription } from './api-tool'
import { getRAGContext, hasKnowledge } from '@/lib/rag/retriever'
import { getDefaultModel } from '@/lib/llm/models'
import type { LLMProvider } from '@/lib/llm/models'
import type { DeployedAgent, AgentTask } from '@/types/database'

export interface ExecutionResult {
  success: boolean
  output: string
  sources: string[]
  toolsUsed: string[]
  error?: string
}

/**
 * Execute an agent task with MCP tools using tool calling
 */
export async function executeAgentWithTools(
  agent: DeployedAgent,
  task: AgentTask
): Promise<ExecutionResult> {
  try {
    // Determine which tools the agent can use
    const agentCapabilities = agent.capabilities || []
    const enabledTools: MCPToolName[] = []

    // Helper function to check if any capability matches (case-insensitive, supports Korean)
    const hasCapability = (...keywords: string[]) => {
      return agentCapabilities.some(cap => {
        const capLower = cap.toLowerCase()
        return keywords.some(kw => capLower.includes(kw.toLowerCase()))
      })
    }

    // Map capabilities to tools (supports English and Korean capability names)
    if (hasCapability('web_search', 'research', 'ê²€ìƒ‰', 'ë¦¬ì„œì¹˜', 'ì¡°ì‚¬')) {
      enabledTools.push('web_search')
    }
    if (hasCapability('youtube', 'youtube_transcript', 'ìœ íŠœë¸Œ', 'yt', 'ë™ì˜ìƒ', 'ì˜ìƒ ë¶„ì„')) {
      enabledTools.push('youtube_transcript')
    }
    if (hasCapability('web_fetch', 'web_browse', 'ì›¹', 'í¬ë¡¤ë§', 'ìŠ¤í¬ë˜í•‘')) {
      enabledTools.push('web_fetch')
    }
    if (hasCapability('image_search', 'image', 'ì´ë¯¸ì§€', 'ì‚¬ì§„', 'gif', 'ê·¸ë¦¼')) {
      enabledTools.push('image_search')
    }

    // If no specific tools enabled, enable all by default
    const mcpTools = enabledTools.length > 0
      ? getToolsByNames(enabledTools)
      : getToolsByNames(getAllToolNames())

    // Load API connections and create API tools
    const apiConnections = await loadAgentApiConnections(agent.id)
    const apiTools = createAllApiTools(apiConnections)
    const apiToolsDescription = generateApiToolsDescription(apiConnections)

    // Combine MCP tools and API tools
    const tools: DynamicStructuredTool[] = [...mcpTools, ...apiTools]

    console.log(`Agent "${agent.name}" executing with tools:`, tools.map(t => t.name))
    if (apiTools.length > 0) {
      console.log(`  - API tools: ${apiTools.map(t => t.name).join(', ')}`)
    }

    // ğŸ”¥ RAG ì§€ì‹ë² ì´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    let ragContextText = ''
    let ragSources: string[] = []
    const agentHasKnowledge = await hasKnowledge(agent.id)
    if (agentHasKnowledge) {
      console.log(`  - Agent has knowledge base, fetching relevant context...`)
      const ragResult = await getRAGContext(agent.id, task.instructions)
      if (ragResult && ragResult.contextText) {
        ragContextText = ragResult.contextText
        ragSources = ragResult.sourcesUsed || []
        console.log(`  - RAG context retrieved (${ragContextText.length} chars, ${ragSources.length} sources)`)
      }
    }

    // ğŸ”¥ ì—ì´ì „íŠ¸ì˜ LLM ì„¤ì • ì‚¬ìš©
    const provider = (agent.llm_provider || 'openai') as LLMProvider
    let model = agent.model || getDefaultModel(provider)

    // Create LLM with tool support based on provider
    let llm: any
    switch (provider) {
      case 'grok':
        llm = new ChatOpenAI({
          modelName: model,
          temperature: agent.temperature || 0.3,
          apiKey: process.env.XAI_API_KEY,
          configuration: { baseURL: 'https://api.x.ai/v1' },
        })
        break
      case 'gemini':
        llm = new ChatOpenAI({
          modelName: model,
          temperature: agent.temperature || 0.3,
          apiKey: process.env.GOOGLE_API_KEY,
          configuration: { baseURL: 'https://generativelanguage.googleapis.com/v1beta/openai/' },
        })
        break
      case 'ollama':
        llm = new ChatOllama({
          model: model,
          temperature: agent.temperature || 0.3,
          baseUrl: 'http://localhost:11434',
        })
        break
      default:
        // OpenAI or fallback
        if (model.startsWith('gpt-4') && !model.includes('gpt-4o')) {
          model = 'gpt-4o-mini'
        }
        llm = new ChatOpenAI({
          modelName: model,
          temperature: agent.temperature || 0.3,
          openAIApiKey: process.env.OPENAI_API_KEY,
        })
    }

    console.log(`  - LLM: ${provider}/${model}`)

    // Bind tools to LLM
    const llmWithTools = llm.bindTools(tools)

    // Create system prompt with RAG context
    const knowledgeSection = ragContextText ? `
## ğŸ“š ë‚´ ì§€ì‹ë² ì´ìŠ¤
ë‹¤ìŒì€ ì—…ë¬´ì™€ ê´€ë ¨ëœ ë‚´ê°€ ì•Œê³  ìˆëŠ” ì •ë³´ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”:

${ragContextText}

---
` : ''

    const systemPrompt = `${agent.system_prompt || `ë‹¹ì‹ ì€ ${agent.name}ì…ë‹ˆë‹¤.`}
${knowledgeSection}
## ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬
${tools.map(t => `- ${t.name}: ${t.description}`).join('\n')}
${apiToolsDescription}

## ğŸ“‹ í˜„ì¬ ì—…ë¬´
- ì œëª©: ${task.title}
- ì„¤ëª…: ${task.description || 'ì—†ìŒ'}
- ì§€ì‹œì‚¬í•­: ${task.instructions}

## âš ï¸ ì¤‘ìš” ì§€ì¹¨
1. **ì§€ì‹ë² ì´ìŠ¤ ìµœìš°ì„ **: ìœ„ì— ì§€ì‹ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ê·¸ ì •ë³´ë¥¼ ê°€ì¥ ë¨¼ì € í™œìš©í•˜ì„¸ìš”!
2. í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
3. YouTube URLì´ ìˆìœ¼ë©´ youtube_transcript ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ìë§‰ì„ ê°€ì ¸ì˜¤ì„¸ìš”.
4. ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ web_search ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
5. **ì´ë¯¸ì§€/GIF ìš”ì²­ ì‹œ image_search ë„êµ¬ë¥¼ ì‚¬ìš©**í•˜ì„¸ìš”. ë§¤ë²ˆ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ì°¾ìœ¼ì„¸ìš”.
6. ì™¸ë¶€ APIê°€ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ API ë„êµ¬ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
7. ëª¨ë“  ë‹µë³€ì€ ì§€ì‹ë² ì´ìŠ¤ ë˜ëŠ” ë„êµ¬ì—ì„œ ì–»ì€ ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì„¸ìš”.
8. ì¶œì²˜ë¥¼ ë°˜ë“œì‹œ ëª…ì‹œí•˜ì„¸ìš” (ì§€ì‹ë² ì´ìŠ¤ ì¶œì²˜ í¬í•¨).
9. ì ˆëŒ€ë¡œ ì •ë³´ë¥¼ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.`

    const messages: BaseMessage[] = [
      new SystemMessage(systemPrompt),
      new HumanMessage(task.instructions),
    ]

    const sources: string[] = [...ragSources]  // RAG ì¶œì²˜ ë¨¼ì € ì¶”ê°€
    const toolsUsed: string[] = ragContextText ? ['knowledge_base'] : []  // ì§€ì‹ë² ì´ìŠ¤ ì‚¬ìš© ì‹œ ê¸°ë¡

    // Run agent loop with tool calling
    let iterations = 0
    const maxIterations = 5

    while (iterations < maxIterations) {
      iterations++

      const response = await llmWithTools.invoke(messages)

      // Check if there are tool calls
      const toolCalls = response.tool_calls || []

      if (toolCalls.length === 0) {
        // No more tool calls, we have the final answer
        let output = typeof response.content === 'string'
          ? response.content
          : 'ì‘ì—…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.'

        // Append sources
        if (sources.length > 0) {
          output += '\n\n---\nğŸ“ ì¶œì²˜:\n'
          const uniqueSources = Array.from(new Set(sources))
          uniqueSources.forEach((src, idx) => {
            output += `${idx + 1}. ${src}\n`
          })
        }

        // Append tools used
        if (toolsUsed.length > 0) {
          output += `\nğŸ”§ ì‚¬ìš©í•œ ë„êµ¬: ${toolsUsed.join(', ')}`
        }

        return {
          success: true,
          output,
          sources: Array.from(new Set(sources)),
          toolsUsed,
        }
      }

      // Process tool calls
      messages.push(response as AIMessage)

      for (const toolCall of toolCalls) {
        const toolName = toolCall.name
        const toolArgs = toolCall.args as Record<string, unknown>

        console.log(`Calling tool: ${toolName}`, toolArgs)

        if (!toolsUsed.includes(toolName)) {
          toolsUsed.push(toolName)
        }

        // Execute the tool - first check ALL_TOOLS (MCP tools), then check dynamically created tools
        let tool = ALL_TOOLS[toolName as MCPToolName]
        if (!tool) {
          // Look for the tool in the combined tools array (includes API tools)
          tool = tools.find(t => t.name === toolName) as any
        }

        let toolResult = ''

        if (tool) {
          try {
            // Execute the tool using func
            toolResult = await tool.func(toolArgs as any)

            // Try to extract sources from tool result
            try {
              const parsed = JSON.parse(toolResult)
              if (parsed.sources) {
                sources.push(...parsed.sources)
              }
              if (parsed.url) {
                sources.push(parsed.url)
              }
              if (parsed.videoUrl) {
                sources.push(parsed.videoUrl)
              }
              // API ë„êµ¬ì—ì„œ dataì— URLì´ ìˆìœ¼ë©´ ì¶”ê°€
              if (parsed.data?.url) {
                sources.push(parsed.data.url)
              }
            } catch {
              // Not JSON, skip
            }
          } catch (error) {
            toolResult = JSON.stringify({ error: `ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: ${error}` })
          }
        } else {
          toolResult = JSON.stringify({ error: `ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: ${toolName}` })
        }

        // Add tool result as ToolMessage with tool_call_id
        messages.push(new ToolMessage({
          content: toolResult,
          tool_call_id: toolCall.id || `call_${toolName}_${Date.now()}`,
          name: toolName,
        }))
      }
    }

    // Max iterations reached
    return {
      success: true,
      output: 'ì‘ì—…ì„ ì™„ë£Œí–ˆì§€ë§Œ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.',
      sources: Array.from(new Set(sources)),
      toolsUsed,
    }
  } catch (error) {
    console.error('Agent execution error:', error)
    return {
      success: false,
      output: '',
      sources: [],
      toolsUsed: [],
      error: error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜',
    }
  }
}
