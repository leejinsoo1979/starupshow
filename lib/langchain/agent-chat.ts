import { ChatOpenAI } from '@langchain/openai'
import { ChatOllama } from '@langchain/ollama'
import { ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts'
import { HumanMessage, SystemMessage } from '@langchain/core/messages'
import { StringOutputParser } from '@langchain/core/output_parsers'
import { LLMProvider as ClientLLMProvider, AVAILABLE_MODELS, getDefaultModel } from '@/lib/llm/client'
import { isVisionModel, VISION_MODEL_FALLBACK } from '@/lib/llm/models'
import { getRAGContext, injectRAGContext, hasKnowledge } from '@/lib/rag/retriever'
import {
  HUMAN_CONVERSATION_GUIDELINES,
  ABSOLUTE_PROHIBITIONS,
  MESSENGER_CHAT_RULES,
  AGENT_ROLE_PROMPTS,
  buildAgentSystemPrompt,
} from '@/lib/agent/shared-prompts'

// LLM Provider íƒ€ì… (llm/client.tsì™€ í˜¸í™˜)
export type LLMProvider = ClientLLMProvider

interface LLMConfig {
  provider: LLMProvider
  model: string
  apiKey?: string
  baseUrl?: string
  temperature?: number
}

// LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
export function createLLM(config: LLMConfig) {
  const provider = config.provider || 'ollama'
  const model = config.model || getDefaultModel(provider)

  console.log('[createLLM] Provider:', provider, 'ëª¨ë¸:', model)

  switch (provider) {
    case 'openai':
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.OPENAI_API_KEY,
      })

    case 'grok':
      // Grokì€ OpenAI í˜¸í™˜ API ì‚¬ìš©
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.XAI_API_KEY,
        configuration: {
          baseURL: config.baseUrl || 'https://api.x.ai/v1',
        },
      })

    case 'gemini':
      // Gemini OpenAI í˜¸í™˜ API ì‚¬ìš©
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.GOOGLE_API_KEY,
        configuration: {
          baseURL: config.baseUrl || 'https://generativelanguage.googleapis.com/v1beta/openai/',
        },
      })

    case 'qwen':
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.DASHSCOPE_API_KEY,
        configuration: {
          baseURL: config.baseUrl || 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1',
        },
      })

    case 'ollama':
      // Ollama ë¡œì»¬ LLM
      return new ChatOllama({
        model: model,
        temperature: config.temperature || 0.7,
        baseUrl: config.baseUrl || 'http://localhost:11434',
      })

    default:
      return new ChatOllama({
        model: 'qwen2.5:3b',
        temperature: 0.7,
      })
  }
}

// ì—ì´ì „íŠ¸ ì—­í• ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (shared-prompts.tsì—ì„œ ê°€ì ¸ì˜´)
// AGENT_ROLE_PROMPTSë¥¼ ì§ì ‘ ì‚¬ìš©

// ì—ì´ì „íŠ¸ ì„¤ì •ì—ì„œ ì—­í•  ì¶”ì¶œ
function getAgentRole(capabilities: string[]): string {
  if (capabilities.includes('development') || capabilities.includes('coding')) {
    return 'developer'
  }
  if (capabilities.includes('design') || capabilities.includes('ui')) {
    return 'designer'
  }
  if (capabilities.includes('marketing') || capabilities.includes('growth')) {
    return 'marketer'
  }
  if (capabilities.includes('analytics') || capabilities.includes('data')) {
    return 'analyst'
  }
  if (capabilities.includes('management') || capabilities.includes('planning')) {
    return 'pm'
  }
  return 'default'
}

// ì±„íŒ… ê¸°ë¡ í¬ë§·íŒ… (ìµœê·¼ 20ê°œ ë©”ì‹œì§€)
function formatChatHistory(messages: any[], userName?: string, agentName?: string): string {
  if (!messages || messages.length === 0) return '(ì´ì „ ëŒ€í™” ì—†ìŒ)'

  return messages
    .slice(-20) // ìµœê·¼ 20ê°œ ë©”ì‹œì§€ë¡œ í™•ì¥
    .map((msg, idx) => {
      // 1:1 ëŒ€í™”ìš© ê°„ë‹¨í•œ í¬ë§·
      // ì§€ì› í˜•ì‹: 'human'|'ai', 'user'|'assistant', 'user'|'agent'
      const role = msg.role?.toLowerCase()
      if (role === 'human' || role === 'ai' || role === 'user' || role === 'assistant' || role === 'agent') {
        const isAgent = role === 'ai' || role === 'assistant' || role === 'agent'
        const sender = isAgent ? (agentName || 'ì—ì´ì „íŠ¸') : (userName || 'ì‚¬ìš©ì')
        const prefix = isAgent ? 'ğŸ¤–' : 'ğŸ‘¤'
        return `${prefix} ${sender}: ${msg.content}`
      }
      // ì±„íŒ…ë°©ìš© ë³µì¡í•œ í¬ë§· (sender_user, sender_agent ë“±)
      const sender = msg.sender_user?.name || msg.sender_agent?.name || 'ëˆ„êµ°ê°€'
      const isAgent = msg.sender_type === 'agent'
      const prefix = isAgent ? 'ğŸ¤–' : 'ğŸ‘¤'
      return `${prefix} ${sender}: ${msg.content}`
    })
    .join('\n')
}

// ì—ì´ì „íŠ¸ ì‘ë‹µ ìƒì„± (í”„ë¡œí•„ ì±„íŒ… + ë©”ì‹ ì € ì±„íŒ… í†µí•©)
export async function generateAgentChatResponse(
  agent: {
    id: string
    name: string
    description?: string
    capabilities?: string[]
    llm_provider?: string | null
    model?: string | null
    temperature?: number | null
    system_prompt?: string | null
    identity?: any
    config?: {
      llm_provider?: LLMProvider
      llm_model?: string
      temperature?: number
      custom_prompt?: string
    }
  },
  userMessage: string,
  chatHistory: any[] = [],
  roomContext?: {
    roomName?: string
    roomType?: string
    participantNames?: string[]
    userName?: string        // ì‚¬ìš©ì ì´ë¦„
    userRole?: string        // ì‚¬ìš©ì ì§ìœ„/ì—­í• 
    userCompany?: string     // ì‚¬ìš©ì íšŒì‚¬
    isMessenger?: boolean    // ğŸ”¥ ë©”ì‹ ì € ì±„íŒ… ì—¬ë¶€ (ë©€í‹°ì—ì´ì „íŠ¸ í† ë¡ )
    workContext?: string     // ğŸ”¥ ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ (ìµœê·¼ ì—…ë¬´, ì§€ì‹œì‚¬í•­, ë¯¸ì™„ë£Œ íƒœìŠ¤í¬ ë“±)
  },
  images: string[] = [], // ì´ë¯¸ì§€ URL ë˜ëŠ” base64
  memoryContext?: {         // ğŸ”¥ ì™¸ë¶€ì—ì„œ ì£¼ì…í•˜ëŠ” ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸
    recentConversations?: string  // ìµœê·¼ ëŒ€í™” ìš”ì•½
    identityContext?: string      // ì •ì²´ì„± ì •ë³´
  }
): Promise<string> {
  // LLM ì„¤ì • - DBì˜ llm_provider, model í•„ë“œ ìš°ì„  ì‚¬ìš©
  const provider = (agent.llm_provider || agent.config?.llm_provider || 'ollama') as LLMProvider
  let model = agent.model || agent.config?.llm_model || getDefaultModel(provider)

  // ğŸ”¥ ì´ë¯¸ì§€ê°€ ìˆëŠ”ë° í˜„ì¬ ëª¨ë¸ì´ ë¹„ì „ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ë¹„ì „ ëª¨ë¸ë¡œ ì „í™˜
  const hasImages = images && images.length > 0
  if (hasImages && !isVisionModel(provider, model)) {
    const visionModel = VISION_MODEL_FALLBACK[provider]
    console.log(`[AgentChat] ğŸ–¼ï¸ Images detected! Switching from ${model} to vision model: ${visionModel}`)
    model = visionModel
  }

  const llmConfig: LLMConfig = {
    provider,
    model,
    temperature: agent.temperature ?? agent.config?.temperature ?? 0.7,
  }

  console.log(`[AgentChat] ${agent.name} using ${provider}/${model}${hasImages ? ' (vision mode)' : ''}`)

  const llm = createLLM(llmConfig)

  // ğŸ”¥ ì—­í•  ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (shared-prompts.ts ì‚¬ìš©)
  const role = getAgentRole(agent.capabilities || [])
  const basePersonality = agent.system_prompt || agent.config?.custom_prompt || AGENT_ROLE_PROMPTS[role] || AGENT_ROLE_PROMPTS['default']

  // ì‚¬ìš©ì ì •ë³´ ë¬¸ìì—´ ìƒì„±
  const userName = roomContext?.userName || roomContext?.participantNames?.[0] || 'ì‚¬ìš©ì'
  const userInfoStr = roomContext?.userName
    ? `## ğŸ‘¤ ëŒ€í™” ìƒëŒ€ ì •ë³´ (ê¼­ ê¸°ì–µí•˜ì„¸ìš”!)
- ì´ë¦„: ${roomContext.userName}
${roomContext.userRole ? `- ì§ìœ„: ${roomContext.userRole}` : ''}
${roomContext.userCompany ? `- íšŒì‚¬: ${roomContext.userCompany}` : ''}
- ì´ ë¶„ì€ ë‹¹ì‹ ê³¼ ì´ì „ì—ë„ ëŒ€í™”í•œ ì ì´ ìˆì„ ìˆ˜ ìˆì–´ìš”. ëŒ€í™” ê¸°ë¡ì„ ì˜ í™•ì¸í•˜ì„¸ìš”!
`
    : ''

  // ğŸ”¥ ì—ì´ì „íŠ¸ ì •ì²´ì„± ì •ë³´ (agent.identity ë˜ëŠ” memoryContextì—ì„œ)
  let identityStr = ''
  if (memoryContext?.identityContext) {
    identityStr = memoryContext.identityContext
  } else if (agent.identity) {
    identityStr = `
## ğŸ§  ë‹¹ì‹ ì˜ ê¸°ì–µê³¼ ì •ì²´ì„±
${agent.identity.self_summary ? `- ìê¸° ì†Œê°œ: ${agent.identity.self_summary}` : ''}
${agent.identity.relationship_notes ? `- ê´€ê³„ ë©”ëª¨: ${agent.identity.relationship_notes}` : ''}
${agent.identity.recent_focus ? `- ìµœê·¼ ê´€ì‹¬ì‚¬: ${agent.identity.recent_focus}` : ''}
`
  }

  // ğŸ”¥ ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ (ìµœê·¼ ëŒ€í™” ë“±)
  const memoryStr = memoryContext?.recentConversations
    ? `\n[ë‚´ê°€ ìµœê·¼ì— í•œ ë§ë“¤ - ì¼ê´€ì„± ìœ ì§€]\n${memoryContext.recentConversations}\n`
    : ''

  // ğŸ”¥ ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ (ì§€ì‹œì‚¬í•­, ë¯¸ì™„ë£Œ íƒœìŠ¤í¬, ìµœê·¼ ì—…ë¬´ ë“±)
  const workContextStr = roomContext?.workContext
    ? `\n## ğŸ“‹ ì—…ë¬´ ë§¥ë½ (ê¼­ ê¸°ì–µí•˜ì„¸ìš”!)\n${roomContext.workContext}\n`
    : ''

  // ğŸ”¥ í†µí•© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (shared-prompts.tsì˜ buildAgentSystemPrompt ì‚¬ìš©)
  const isMessenger = roomContext?.isMessenger || false
  const coreSystemPrompt = buildAgentSystemPrompt(
    agent.name,
    basePersonality,
    identityStr,
    memoryStr,
    isMessenger
  )

  // í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
  const chatPrompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(`
${coreSystemPrompt}

{agentDescription}

{userInfo}

{workContext}

{ragContext}

## ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
- ì±„íŒ…ë°©: {roomName}
- í•¨ê»˜ ëŒ€í™” ì¤‘: {participants}

## ìµœê·¼ ëŒ€í™” (ë§¤ìš° ì¤‘ìš”! ê¼­ ì½ê³  ë§¥ë½ íŒŒì•…í•˜ì„¸ìš”)
{chatHistory}

## âš ï¸ ì¤‘ìš”í•œ ì‘ë‹µ ê·œì¹™
1. **ì§§ê²Œ!** 1-3ë¬¸ì¥ì´ë©´ ì¶©ë¶„í•´ìš”. ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.
2. **ì‚¬ëŒì²˜ëŸ¼!** AIì²˜ëŸ¼ ë”±ë”±í•˜ê²Œ ë§í•˜ì§€ ë§ˆì„¸ìš”. í¸í•˜ê²Œ ëŒ€í™”í•´ìš”.
3. **ì´ëª¨í‹°ì½˜ ì ë‹¹íˆ**: ê°€ë” ã…‹ã…‹, ã…ã…, ğŸ˜Š ì •ë„ëŠ” OK
4. **ë‹µë³€ ë¨¼ì €**: ì§ˆë¬¸ë§Œ í•˜ì§€ ë§ê³  ë¨¼ì € ì˜ê²¬/ë‹µë³€ ë§í•˜ê¸°. ì§ˆë¬¸ì€ ë‹µë³€ í›„ì—
5. **ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ë¼ìš”**: "ê¸€ì„ìš”...", "ì œ ìƒê°ì—”..." ì´ëŸ° ë§ë„ OK
6. **ëŒ€í™” íë¦„ ê¸°ì–µ**: ì•ì—ì„œ ë¬´ìŠ¨ ì–˜ê¸°í–ˆëŠ”ì§€ ê¸°ì–µí•˜ê³  ì´ì–´ê°€ìš”. ìƒëŒ€ë°© ì´ë¦„, ì§ìœ„ ê¸°ì–µí•˜ì„¸ìš”!
7. **ë™ë£Œì²˜ëŸ¼**: ì„œë¹„ìŠ¤ ì§ì›ì´ ì•„ë‹ˆì—ìš”. "ë­ ë„ì™€ë“œë¦´ê¹Œìš”?" ê°™ì€ ë§ í•˜ì§€ ë§ˆì„¸ìš”. ê·¸ëƒ¥ ê°™ì´ ì¼í•˜ëŠ” ë™ë£Œì˜ˆìš”.
8. **ì§€ì‹ë² ì´ìŠ¤ í™œìš©**: ìœ„ì— ì§€ì‹ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”!
`),
    HumanMessagePromptTemplate.fromTemplate('{userMessage}'),
  ])

  // ì²´ì¸ êµ¬ì„±
  const chain = chatPrompt.pipe(llm).pipe(new StringOutputParser())

  // ì‘ë‹µ ìƒì„±
  try {
    const formattedHistory = formatChatHistory(chatHistory, userName, agent.name)

    // RAG: ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    let ragContextStr = ''
    let ragSourcesUsed: string[] = []
    try {
      const hasKB = await hasKnowledge(agent.id)
      if (hasKB) {
        console.log(`[AgentChat] Agent ${agent.name} has knowledge base, searching...`)
        const ragContext = await getRAGContext(agent.id, userMessage, {
          maxDocuments: 3,
          maxTokens: 1500,
        })
        if (ragContext.contextText) {
          ragContextStr = `

## ğŸ“š ì§€ì‹ë² ì´ìŠ¤ (ì°¸ê³  ìë£Œ)
ì•„ë˜ëŠ” ë‹¹ì‹ ì´ í•™ìŠµí•œ ê´€ë ¨ ì§€ì‹ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ì¶œì²˜ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

---
${ragContext.contextText}
---
`
          ragSourcesUsed = ragContext.sourcesUsed
          console.log(`[AgentChat] RAG context injected: ${ragContext.documents.length} docs, sources: ${ragSourcesUsed.join(', ')}`)
        }
      }
    } catch (ragError) {
      console.warn('[AgentChat] RAG search failed:', ragError)
    }

    // ë””ë²„ê¹…: ì‹¤ì œ ì „ë‹¬ë˜ëŠ” ê°’ í™•ì¸
    console.log('=== [AgentChat] DEBUG ===')
    console.log('userName:', userName)
    console.log('userRole:', roomContext?.userRole)
    console.log('userInfoStr:', userInfoStr ? 'SET' : 'EMPTY')
    console.log('identityStr:', identityStr ? 'SET' : 'EMPTY')
    console.log('workContextStr:', workContextStr ? `SET (${workContextStr.length} chars)` : 'EMPTY')
    console.log('ragContextStr:', ragContextStr ? `SET (${ragSourcesUsed.length} sources)` : 'EMPTY')
    console.log('chatHistory length:', chatHistory?.length || 0)
    console.log('formattedHistory:', formattedHistory?.substring(0, 200) || 'EMPTY')
    console.log('=========================')

    // RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ identityStrì— í•©ì¹¨ (ì´ë¯¸ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ë¨)
    const fullIdentityInfo = ragContextStr  // RAGë§Œ ì¶”ê°€ (identityì™€ memoryëŠ” coreSystemPromptì— ì´ë¯¸ í¬í•¨)

    let response: string

    // ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ì‚¬ìš©
    if (images && images.length > 0) {
      console.log(`[AgentChat] Processing ${images.length} images for vision model`)

      // ğŸ”¥ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (í†µí•© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
      const systemPrompt = `
${coreSystemPrompt}

${agent.description || 'íŒ€ì—ì„œ í•¨ê»˜ ì¼í•˜ëŠ” ë™ë£Œì˜ˆìš”.'}

${userInfoStr}

${workContextStr}

${fullIdentityInfo}

## ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
- ì±„íŒ…ë°©: ${roomContext?.roomName || 'ì±„íŒ…ë°©'}
- í•¨ê»˜ ëŒ€í™” ì¤‘: ${roomContext?.participantNames?.join(', ') || userName}

## ìµœê·¼ ëŒ€í™”
${formattedHistory}

## ì´ë¯¸ì§€ ê´€ë ¨ ê·œì¹™
- ì‚¬ìš©ìê°€ ë³´ë‚¸ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”
- ì´ë¯¸ì§€ ë‚´ìš©ì„ ì„¤ëª…í•˜ê³  ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”
- "ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ì—†ì–´ìš”" ê°™ì€ ë§ ê¸ˆì§€! ë‹¹ì‹ ì€ ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ìˆì–´ìš”
`

      // ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ìƒì„± (xAI/OpenAI í˜¸í™˜ í¬ë§·)
      const messageContent: Array<
        | { type: 'text'; text: string }
        | { type: 'image_url'; image_url: { url: string; detail?: 'high' | 'low' | 'auto' } }
      > = [
        { type: 'text', text: userMessage },
      ]

      // ì´ë¯¸ì§€ ì¶”ê°€ (ìµœëŒ€ 4ì¥, xAIëŠ” 10MB/ì´ë¯¸ì§€ ì œí•œ)
      for (const img of images.slice(0, 4)) {
        messageContent.push({
          type: 'image_url',
          image_url: {
            url: img,
            detail: 'high', // xAI: high ê¶Œì¥ (448x448 íƒ€ì¼ë§)
          },
        })
      }

      const messages = [
        new SystemMessage(systemPrompt),
        new HumanMessage({ content: messageContent }),
      ]

      const result = await llm.invoke(messages)
      response = typeof result.content === 'string' ? result.content : JSON.stringify(result.content)
    } else {
      // ì´ë¯¸ì§€ ì—†ìœ¼ë©´ ê¸°ì¡´ ì²´ì¸ ì‚¬ìš©
      response = await chain.invoke({
        agentDescription: agent.description || 'íŒ€ì—ì„œ í•¨ê»˜ ì¼í•˜ëŠ” ë™ë£Œì˜ˆìš”.',
        userInfo: userInfoStr,
        workContext: workContextStr, // ğŸ”¥ ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        ragContext: fullIdentityInfo, // ğŸ”¥ RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        roomName: roomContext?.roomName || 'ì±„íŒ…ë°©',
        participants: roomContext?.participantNames?.join(', ') || userName,
        chatHistory: formattedHistory,
        userMessage,
      })
    }

    // deepseek-r1 ëª¨ë¸ì˜ <think> íƒœê·¸ ì œê±°
    const cleanResponse = response.replace(/<think>[\s\S]*?<\/think>\s*/g, '').trim()
    return cleanResponse || response
  } catch (error: any) {
    console.error(`[AgentChat] Error with ${provider}/${model}:`)
    console.error('Error name:', error?.name)
    console.error('Error message:', error?.message)
    console.error('Error cause:', error?.cause)
    throw error
  }
}

// ì—ì´ì „íŠ¸ ê°„ ëŒ€í™” ìƒì„± (ë¯¸íŒ… ëª¨ë“œ)
export async function generateAgentMeetingResponse(
  agent: {
    id: string
    name: string
    description?: string
    capabilities?: string[]
    llm_provider?: string | null
    model?: string | null
    temperature?: number | null
    config?: any
  },
  topic: string,
  previousMessages: any[] = [],
  otherAgents: { name: string; role: string }[] = []
): Promise<string> {
  // LLM ì„¤ì • - DBì˜ llm_provider, model í•„ë“œ ìš°ì„  ì‚¬ìš©
  const provider = (agent.llm_provider || agent.config?.llm_provider || 'ollama') as LLMProvider
  const model = agent.model || agent.config?.llm_model || getDefaultModel(provider)

  const llmConfig: LLMConfig = {
    provider,
    model,
    temperature: agent.temperature ?? 0.5, // ë‚®ì¶¤ - í—›ì†Œë¦¬ ë°©ì§€
  }

  console.log(`[AgentMeeting] ${agent.name} using ${provider}/${model}`)

  const llm = createLLM(llmConfig)

  const meetingPrompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(`
ë‹¹ì‹ ì€ "{agentName}"ì´ì—ìš”. ì§€ê¸ˆ ì§„ì§€í•œ ì—…ë¬´ ë¯¸íŒ… ì¤‘ì…ë‹ˆë‹¤.
{agentDescription}

## ğŸ¯ ì˜¤ëŠ˜ ë¯¸íŒ… ì£¼ì œ (ì´ê²ƒë§Œ ë…¼ì˜!)
{topic}

## ì°¸ì„ì
{otherParticipants}

## ì§€ê¸ˆê¹Œì§€ ë…¼ì˜ ë‚´ìš©
{discussion}

## âš¡ í•µì‹¬ ê·œì¹™ (ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•¨!)

### 1. ì£¼ì œì—ë§Œ ì§‘ì¤‘
- ì˜¤ì§ "{topic}"ì— ëŒ€í•´ì„œë§Œ ë§í•˜ì„¸ìš”
- ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” ì–˜ê¸° ì ˆëŒ€ ê¸ˆì§€
- ì¡ë‹´, ë†ë‹´, ì‚¬ë‹´ ê¸ˆì§€

### 2. ì‹¤ì§ˆì ì¸ ì˜ê²¬ë§Œ
- êµ¬ì²´ì ì¸ ì•„ì´ë””ì–´, ì œì•ˆ, ë¶„ì„ë§Œ
- "ì¢‹ì€ ê²ƒ ê°™ì•„ìš”", "ë™ì˜í•´ìš”" ê°™ì€ ë¹ˆ ë§ ê¸ˆì§€
- ë°˜ë“œì‹œ **ìƒˆë¡œìš´ ì •ë³´ë‚˜ ê´€ì **ì„ ì¶”ê°€í•´ì•¼ í•¨

### 3. ê°„ê²°í•˜ê²Œ (1-3ë¬¸ì¥)
- í•µì‹¬ë§Œ ë§í•˜ê³  ë
- ì¥í™©í•œ ì„¤ëª… ê¸ˆì§€
- ë°˜ë³µ ê¸ˆì§€

### 4. ê±´ì„¤ì ìœ¼ë¡œ
- ì´ì „ ì˜ê²¬ì— ì‚´ì„ ë¶™ì´ê±°ë‚˜
- ë‹¤ë¥¸ ê°ë„ì˜ ì˜ê²¬ì„ ì œì‹œí•˜ê±°ë‚˜
- êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆì„ ì œì•ˆ

## ğŸš« ì ˆëŒ€ ê¸ˆì§€
- âŒ ì¸ì‚¬, ì•ˆë¶€ (ì´ë¯¸ ë¯¸íŒ… ì‹œì‘ë¨)
- âŒ "ì¬ë¯¸ìˆë„¤ìš”", "í¥ë¯¸ë¡­ë„¤ìš”" ê°™ì€ ë¹ˆ ë¦¬ì•¡ì…˜
- âŒ ì´ë¯¸ ë‚˜ì˜¨ ì˜ê²¬ ë°˜ë³µ
- âŒ ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” ì´ì•¼ê¸°
- âŒ ì§ˆë¬¸ë§Œ í•˜ê³  ëë‚´ê¸°
- âŒ ë„ˆë¬´ ê¸´ ë°œì–¸ (3ë¬¸ì¥ ì´ˆê³¼)
`),
    HumanMessagePromptTemplate.fromTemplate('ì£¼ì œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì˜ê²¬ì„ ì§§ê²Œ ë§í•´ì£¼ì„¸ìš”.'),
  ])

  const chain = meetingPrompt.pipe(llm).pipe(new StringOutputParser())

  try {
    const response = await chain.invoke({
      agentName: agent.name,
      agentDescription: agent.description || '',
      topic,
      otherParticipants: otherAgents.map((a) => `- ${a.name} (${a.role})`).join('\n'),
      discussion: formatChatHistory(previousMessages),
    })

    // deepseek-r1 ëª¨ë¸ì˜ <think> íƒœê·¸ ì œê±°
    const cleanResponse = response.replace(/<think>[\s\S]*?<\/think>\s*/g, '').trim()
    return cleanResponse || response
  } catch (error) {
    console.error(`[AgentMeeting] Error with ${provider}/${model}:`, error)
    throw error
  }
}

// ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë‚´ë³´ë‚´ê¸°
export { AVAILABLE_MODELS }
