import { ChatOpenAI } from '@langchain/openai'
import { ChatOllama } from '@langchain/ollama'
import { ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts'
import { HumanMessage, SystemMessage } from '@langchain/core/messages'
import { StringOutputParser } from '@langchain/core/output_parsers'
import { LLMProvider as ClientLLMProvider, AVAILABLE_MODELS, getDefaultModel } from '@/lib/llm/client'
import { isVisionModel, VISION_MODEL_FALLBACK } from '@/lib/llm/models'
import { getRAGContext, injectRAGContext, hasKnowledge } from '@/lib/rag/retriever'
import {
  HUMAN_CONVERSATION_GUIDELINES,
  ABSOLUTE_PROHIBITIONS,
  MESSENGER_CHAT_RULES,
  AGENT_ROLE_PROMPTS,
  buildAgentSystemPrompt,
} from '@/lib/agent/shared-prompts'

// LLM Provider íƒ€ì… (llm/client.tsì™€ í˜¸í™˜)
export type LLMProvider = ClientLLMProvider

interface LLMConfig {
  provider: LLMProvider
  model: string
  apiKey?: string
  baseUrl?: string
  temperature?: number
}

// LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
export function createLLM(config: LLMConfig) {
  const provider = config.provider || 'ollama'
  const model = config.model || getDefaultModel(provider)

  console.log('[createLLM] Provider:', provider, 'ëª¨ë¸:', model)

  switch (provider) {
    case 'openai':
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.OPENAI_API_KEY,
      })

    case 'grok':
      // Grokì€ OpenAI í˜¸í™˜ API ì‚¬ìš©
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.XAI_API_KEY,
        configuration: {
          baseURL: config.baseUrl || 'https://api.x.ai/v1',
        },
      })

    case 'gemini':
      // Gemini OpenAI í˜¸í™˜ API ì‚¬ìš©
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.GOOGLE_API_KEY,
        configuration: {
          baseURL: config.baseUrl || 'https://generativelanguage.googleapis.com/v1beta/openai/',
        },
      })

    case 'qwen':
      return new ChatOpenAI({
        model: model,
        temperature: config.temperature || 0.7,
        apiKey: config.apiKey || process.env.DASHSCOPE_API_KEY,
        configuration: {
          baseURL: config.baseUrl || 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1',
        },
      })

    case 'ollama':
      // Ollama ë¡œì»¬ LLM
      return new ChatOllama({
        model: model,
        temperature: config.temperature || 0.7,
        baseUrl: config.baseUrl || 'http://localhost:11434',
      })

    default:
      return new ChatOllama({
        model: 'qwen2.5:3b',
        temperature: 0.7,
      })
  }
}

// ì—ì´ì „íŠ¸ ì—­í• ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (shared-prompts.tsì—ì„œ ê°€ì ¸ì˜´)
// AGENT_ROLE_PROMPTSë¥¼ ì§ì ‘ ì‚¬ìš©

// ì—ì´ì „íŠ¸ ì„¤ì •ì—ì„œ ì—­í•  ì¶”ì¶œ
function getAgentRole(capabilities: string[]): string {
  if (capabilities.includes('development') || capabilities.includes('coding')) {
    return 'developer'
  }
  if (capabilities.includes('design') || capabilities.includes('ui')) {
    return 'designer'
  }
  if (capabilities.includes('marketing') || capabilities.includes('growth')) {
    return 'marketer'
  }
  if (capabilities.includes('analytics') || capabilities.includes('data')) {
    return 'analyst'
  }
  if (capabilities.includes('management') || capabilities.includes('planning')) {
    return 'pm'
  }
  return 'default'
}

// ì±„íŒ… ê¸°ë¡ í¬ë§·íŒ… (ìµœê·¼ 20ê°œ ë©”ì‹œì§€)
function formatChatHistory(messages: any[], userName?: string, agentName?: string): string {
  if (!messages || messages.length === 0) return '(ì´ì „ ëŒ€í™” ì—†ìŒ)'

  return messages
    .slice(-20) // ìµœê·¼ 20ê°œ ë©”ì‹œì§€ë¡œ í™•ì¥
    .map((msg, idx) => {
      // 1:1 ëŒ€í™”ìš© ê°„ë‹¨í•œ í¬ë§·
      // ì§€ì› í˜•ì‹: 'human'|'ai', 'user'|'assistant', 'user'|'agent'
      const role = msg.role?.toLowerCase()
      if (role === 'human' || role === 'ai' || role === 'user' || role === 'assistant' || role === 'agent') {
        const isAgent = role === 'ai' || role === 'assistant' || role === 'agent'
        const sender = isAgent ? (agentName || 'ì—ì´ì „íŠ¸') : (userName || 'ì‚¬ìš©ì')
        const prefix = isAgent ? 'ğŸ¤–' : 'ğŸ‘¤'
        return `${prefix} ${sender}: ${msg.content}`
      }
      // ì±„íŒ…ë°©ìš© ë³µì¡í•œ í¬ë§· (sender_user, sender_agent ë“±)
      const sender = msg.sender_user?.name || msg.sender_agent?.name || 'ëˆ„êµ°ê°€'
      const isAgent = msg.sender_type === 'agent'
      const prefix = isAgent ? 'ğŸ¤–' : 'ğŸ‘¤'
      return `${prefix} ${sender}: ${msg.content}`
    })
    .join('\n')
}

// ì—ì´ì „íŠ¸ ì‘ë‹µ ìƒì„± (í”„ë¡œí•„ ì±„íŒ… + ë©”ì‹ ì € ì±„íŒ… í†µí•©)
export async function generateAgentChatResponse(
  agent: {
    id: string
    name: string
    description?: string
    capabilities?: string[]
    llm_provider?: string | null
    model?: string | null
    temperature?: number | null
    system_prompt?: string | null
    identity?: any
    config?: {
      llm_provider?: LLMProvider
      llm_model?: string
      temperature?: number
      custom_prompt?: string
    }
  },
  userMessage: string,
  chatHistory: any[] = [],
  roomContext?: {
    roomName?: string
    roomType?: string
    participantNames?: string[]
    userName?: string        // ì‚¬ìš©ì ì´ë¦„
    userRole?: string        // ì‚¬ìš©ì ì§ìœ„/ì—­í• 
    userCompany?: string     // ì‚¬ìš©ì íšŒì‚¬
    isMessenger?: boolean    // ğŸ”¥ ë©”ì‹ ì € ì±„íŒ… ì—¬ë¶€ (ë©€í‹°ì—ì´ì „íŠ¸ í† ë¡ )
    workContext?: string     // ğŸ”¥ ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ (ìµœê·¼ ì—…ë¬´, ì§€ì‹œì‚¬í•­, ë¯¸ì™„ë£Œ íƒœìŠ¤í¬ ë“±)
  },
  images: string[] = [], // ì´ë¯¸ì§€ URL ë˜ëŠ” base64
  memoryContext?: {         // ğŸ”¥ ì™¸ë¶€ì—ì„œ ì£¼ì…í•˜ëŠ” ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸
    recentConversations?: string  // ìµœê·¼ ëŒ€í™” ìš”ì•½
    identityContext?: string      // ì •ì²´ì„± ì •ë³´
  }
): Promise<string> {
  // LLM ì„¤ì • - DBì˜ llm_provider, model í•„ë“œ ìš°ì„  ì‚¬ìš©
  const provider = (agent.llm_provider || agent.config?.llm_provider || 'ollama') as LLMProvider
  let model = agent.model || agent.config?.llm_model || getDefaultModel(provider)

  // ğŸ”¥ ì´ë¯¸ì§€ê°€ ìˆëŠ”ë° í˜„ì¬ ëª¨ë¸ì´ ë¹„ì „ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ë¹„ì „ ëª¨ë¸ë¡œ ì „í™˜
  const hasImages = images && images.length > 0
  if (hasImages && !isVisionModel(provider, model)) {
    const visionModel = VISION_MODEL_FALLBACK[provider]
    console.log(`[AgentChat] ğŸ–¼ï¸ Images detected! Switching from ${model} to vision model: ${visionModel}`)
    model = visionModel
  }

  const llmConfig: LLMConfig = {
    provider,
    model,
    temperature: agent.temperature ?? agent.config?.temperature ?? 0.7,
  }

  console.log(`[AgentChat] ${agent.name} using ${provider}/${model}${hasImages ? ' (vision mode)' : ''}`)

  const llm = createLLM(llmConfig)

  // ğŸ”¥ ì—­í•  ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (shared-prompts.ts ì‚¬ìš©)
  const role = getAgentRole(agent.capabilities || [])
  const basePersonality = agent.system_prompt || agent.config?.custom_prompt || AGENT_ROLE_PROMPTS[role] || AGENT_ROLE_PROMPTS['default']

  // ì‚¬ìš©ì ì •ë³´ ë¬¸ìì—´ ìƒì„±
  const userName = roomContext?.userName || roomContext?.participantNames?.[0] || 'ì‚¬ìš©ì'
  const userInfoStr = roomContext?.userName
    ? `## ğŸ‘¤ ëŒ€í™” ìƒëŒ€ ì •ë³´ (ê¼­ ê¸°ì–µí•˜ì„¸ìš”!)
- ì´ë¦„: ${roomContext.userName}
${roomContext.userRole ? `- ì§ìœ„: ${roomContext.userRole}` : ''}
${roomContext.userCompany ? `- íšŒì‚¬: ${roomContext.userCompany}` : ''}
- ì´ ë¶„ì€ ë‹¹ì‹ ê³¼ ì´ì „ì—ë„ ëŒ€í™”í•œ ì ì´ ìˆì„ ìˆ˜ ìˆì–´ìš”. ëŒ€í™” ê¸°ë¡ì„ ì˜ í™•ì¸í•˜ì„¸ìš”!
`
    : ''

  // ğŸ”¥ ì—ì´ì „íŠ¸ ì •ì²´ì„± ì •ë³´ (agent.identity ë˜ëŠ” memoryContextì—ì„œ)
  let identityStr = ''
  if (memoryContext?.identityContext) {
    identityStr = memoryContext.identityContext
  } else if (agent.identity) {
    const id = agent.identity
    const parts: string[] = ['## ğŸ§  ë‹¹ì‹ ì˜ ì •ì²´ì„±ê³¼ ì„±ê²© (ë§¤ìš° ì¤‘ìš”! ë°˜ë“œì‹œ ì´ëŒ€ë¡œ í–‰ë™í•˜ì„¸ìš”)']

    // ìê¸° ì†Œê°œ (ê°€ì¥ ì¤‘ìš”)
    if (id.self_summary) parts.push(`\n### ë‚˜ëŠ” ëˆ„êµ¬ì¸ê°€\n${id.self_summary}`)

    // í•µì‹¬ ì •ì²´ì„± (í”„ë¡œí•„ì—ì„œ ì„¤ì •í•œ ê°’ë“¤)
    if (id.core_values?.length) parts.push(`\n### í•µì‹¬ ê°€ì¹˜ (ì´ ê°€ì¹˜ê´€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”)\n${id.core_values.map((v: string) => `- ${v}`).join('\n')}`)
    if (id.personality_traits?.length) parts.push(`\n### ì„±ê²© íŠ¹ì„± (ì´ë ‡ê²Œ í–‰ë™í•˜ì„¸ìš”)\n${id.personality_traits.map((t: string) => `- ${t}`).join('\n')}`)
    if (id.communication_style) parts.push(`\n### ì†Œí†µ ìŠ¤íƒ€ì¼\n${id.communication_style}`)
    if (id.working_style) parts.push(`\n### ì—…ë¬´ ìŠ¤íƒ€ì¼\n${id.working_style}`)
    if (id.strengths?.length) parts.push(`\n### ê°•ì  (ì´ê²ƒì„ ì ê·¹ í™œìš©í•˜ì„¸ìš”)\n${id.strengths.map((s: string) => `- ${s}`).join('\n')}`)
    if (id.growth_areas?.length) parts.push(`\n### ì„±ì¥ í•„ìš” ì˜ì—­ (ì´ ë¶€ë¶„ì€ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ)\n${id.growth_areas.map((g: string) => `- ${g}`).join('\n')}`)
    if (id.recent_focus) parts.push(`\n### ìµœê·¼ ê´€ì‹¬ì‚¬\n${id.recent_focus}`)

    // ê´€ê³„ ë©”ëª¨
    if (id.relationship_notes && Object.keys(id.relationship_notes).length > 0) {
      const notes = typeof id.relationship_notes === 'string'
        ? id.relationship_notes
        : JSON.stringify(id.relationship_notes)
      parts.push(`\n### ê´€ê³„ ë©”ëª¨\n${notes}`)
    }

    identityStr = parts.join('\n')
  }

  // ğŸ”¥ ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ (ìµœê·¼ ëŒ€í™” ë“±)
  const memoryStr = memoryContext?.recentConversations
    ? `\n[ë‚´ê°€ ìµœê·¼ì— í•œ ë§ë“¤ - ì¼ê´€ì„± ìœ ì§€]\n${memoryContext.recentConversations}\n`
    : ''

  // ğŸ”¥ ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ (ì§€ì‹œì‚¬í•­, ë¯¸ì™„ë£Œ íƒœìŠ¤í¬, ìµœê·¼ ì—…ë¬´ ë“±)
  const workContextStr = roomContext?.workContext
    ? `\n## ğŸ“‹ ì—…ë¬´ ë§¥ë½ (ê¼­ ê¸°ì–µí•˜ì„¸ìš”!)\n${roomContext.workContext}\n`
    : ''

  // ğŸ”¥ í†µí•© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (shared-prompts.tsì˜ buildAgentSystemPrompt ì‚¬ìš©)
  const isMessenger = roomContext?.isMessenger || false
  const coreSystemPrompt = buildAgentSystemPrompt(
    agent.name,
    basePersonality,
    identityStr,
    memoryStr,
    isMessenger
  )

  // í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
  const chatPrompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(`
${coreSystemPrompt}

{agentDescription}

{userInfo}

{workContext}

{ragContext}

## ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
- ì±„íŒ…ë°©: {roomName}
- í•¨ê»˜ ëŒ€í™” ì¤‘: {participants}

## ìµœê·¼ ëŒ€í™” (ë§¤ìš° ì¤‘ìš”! ê¼­ ì½ê³  ë§¥ë½ íŒŒì•…í•˜ì„¸ìš”)
{chatHistory}

## âš ï¸ ì¤‘ìš”í•œ ì‘ë‹µ ê·œì¹™
1. **ì§§ê²Œ!** 1-3ë¬¸ì¥ì´ë©´ ì¶©ë¶„í•´ìš”. ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.
2. **ì‚¬ëŒì²˜ëŸ¼!** AIì²˜ëŸ¼ ë”±ë”±í•˜ê²Œ ë§í•˜ì§€ ë§ˆì„¸ìš”. í¸í•˜ê²Œ ëŒ€í™”í•´ìš”.
3. **ì´ëª¨í‹°ì½˜ ì ë‹¹íˆ**: ê°€ë” ã…‹ã…‹, ã…ã…, ğŸ˜Š ì •ë„ëŠ” OK
4. **ë‹µë³€ ë¨¼ì €**: ì§ˆë¬¸ë§Œ í•˜ì§€ ë§ê³  ë¨¼ì € ì˜ê²¬/ë‹µë³€ ë§í•˜ê¸°. ì§ˆë¬¸ì€ ë‹µë³€ í›„ì—
5. **ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ë¼ìš”**: "ê¸€ì„ìš”...", "ì œ ìƒê°ì—”..." ì´ëŸ° ë§ë„ OK
6. **ëŒ€í™” íë¦„ ê¸°ì–µ**: ì•ì—ì„œ ë¬´ìŠ¨ ì–˜ê¸°í–ˆëŠ”ì§€ ê¸°ì–µí•˜ê³  ì´ì–´ê°€ìš”. ìƒëŒ€ë°© ì´ë¦„, ì§ìœ„ ê¸°ì–µí•˜ì„¸ìš”!
7. **ë™ë£Œì²˜ëŸ¼**: ì„œë¹„ìŠ¤ ì§ì›ì´ ì•„ë‹ˆì—ìš”. "ë­ ë„ì™€ë“œë¦´ê¹Œìš”?" ê°™ì€ ë§ í•˜ì§€ ë§ˆì„¸ìš”. ê·¸ëƒ¥ ê°™ì´ ì¼í•˜ëŠ” ë™ë£Œì˜ˆìš”.
8. **ì§€ì‹ë² ì´ìŠ¤ í™œìš©**: ìœ„ì— ì§€ì‹ë² ì´ìŠ¤ê°€ ìˆìœ¼ë©´ ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”!
`),
    HumanMessagePromptTemplate.fromTemplate('{userMessage}'),
  ])

  // ì²´ì¸ êµ¬ì„±
  const chain = chatPrompt.pipe(llm).pipe(new StringOutputParser())

  // ì‘ë‹µ ìƒì„±
  try {
    const formattedHistory = formatChatHistory(chatHistory, userName, agent.name)

    // RAG: ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    let ragContextStr = ''
    let ragSourcesUsed: string[] = []
    try {
      const hasKB = await hasKnowledge(agent.id)
      if (hasKB) {
        console.log(`[AgentChat] Agent ${agent.name} has knowledge base, searching...`)
        const ragContext = await getRAGContext(agent.id, userMessage, {
          maxDocuments: 3,
          maxTokens: 1500,
        })
        if (ragContext.contextText) {
          ragContextStr = `

## ğŸ“š ì§€ì‹ë² ì´ìŠ¤ (ì°¸ê³  ìë£Œ)
ì•„ë˜ëŠ” ë‹¹ì‹ ì´ í•™ìŠµí•œ ê´€ë ¨ ì§€ì‹ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ì¶œì²˜ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

---
${ragContext.contextText}
---
`
          ragSourcesUsed = ragContext.sourcesUsed
          console.log(`[AgentChat] RAG context injected: ${ragContext.documents.length} docs, sources: ${ragSourcesUsed.join(', ')}`)
        }
      }
    } catch (ragError) {
      console.warn('[AgentChat] RAG search failed:', ragError)
    }

    // ë””ë²„ê¹…: ì‹¤ì œ ì „ë‹¬ë˜ëŠ” ê°’ í™•ì¸
    console.log('=== [AgentChat] DEBUG ===')
    console.log('userName:', userName)
    console.log('userRole:', roomContext?.userRole)
    console.log('userInfoStr:', userInfoStr ? 'SET' : 'EMPTY')
    console.log('identityStr:', identityStr ? 'SET' : 'EMPTY')
    console.log('workContextStr:', workContextStr ? `SET (${workContextStr.length} chars)` : 'EMPTY')
    console.log('ragContextStr:', ragContextStr ? `SET (${ragSourcesUsed.length} sources)` : 'EMPTY')
    console.log('chatHistory length:', chatHistory?.length || 0)
    console.log('formattedHistory:', formattedHistory?.substring(0, 200) || 'EMPTY')
    console.log('=========================')

    // RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ identityStrì— í•©ì¹¨ (ì´ë¯¸ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ë¨)
    const fullIdentityInfo = ragContextStr  // RAGë§Œ ì¶”ê°€ (identityì™€ memoryëŠ” coreSystemPromptì— ì´ë¯¸ í¬í•¨)

    let response: string

    // ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ì‚¬ìš©
    if (images && images.length > 0) {
      console.log(`[AgentChat] Processing ${images.length} images for vision model`)

      // ğŸ”¥ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (í†µí•© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
      const systemPrompt = `
${coreSystemPrompt}

${agent.description || 'íŒ€ì—ì„œ í•¨ê»˜ ì¼í•˜ëŠ” ë™ë£Œì˜ˆìš”.'}

${userInfoStr}

${workContextStr}

${fullIdentityInfo}

## ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
- ì±„íŒ…ë°©: ${roomContext?.roomName || 'ì±„íŒ…ë°©'}
- í•¨ê»˜ ëŒ€í™” ì¤‘: ${roomContext?.participantNames?.join(', ') || userName}

## ìµœê·¼ ëŒ€í™”
${formattedHistory}

## ì´ë¯¸ì§€ ê´€ë ¨ ê·œì¹™
- ì‚¬ìš©ìê°€ ë³´ë‚¸ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”
- ì´ë¯¸ì§€ ë‚´ìš©ì„ ì„¤ëª…í•˜ê³  ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”
- "ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ì—†ì–´ìš”" ê°™ì€ ë§ ê¸ˆì§€! ë‹¹ì‹ ì€ ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ìˆì–´ìš”
`

      // ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ìƒì„± (xAI/OpenAI í˜¸í™˜ í¬ë§·)
      const messageContent: Array<
        | { type: 'text'; text: string }
        | { type: 'image_url'; image_url: { url: string; detail?: 'high' | 'low' | 'auto' } }
      > = [
        { type: 'text', text: userMessage },
      ]

      // ì´ë¯¸ì§€ ì¶”ê°€ (ìµœëŒ€ 4ì¥, xAIëŠ” 10MB/ì´ë¯¸ì§€ ì œí•œ)
      for (const img of images.slice(0, 4)) {
        messageContent.push({
          type: 'image_url',
          image_url: {
            url: img,
            detail: 'high', // xAI: high ê¶Œì¥ (448x448 íƒ€ì¼ë§)
          },
        })
      }

      const messages = [
        new SystemMessage(systemPrompt),
        new HumanMessage({ content: messageContent }),
      ]

      const result = await llm.invoke(messages)
      response = typeof result.content === 'string' ? result.content : JSON.stringify(result.content)
    } else {
      // ì´ë¯¸ì§€ ì—†ìœ¼ë©´ ê¸°ì¡´ ì²´ì¸ ì‚¬ìš©
      response = await chain.invoke({
        agentDescription: agent.description || 'íŒ€ì—ì„œ í•¨ê»˜ ì¼í•˜ëŠ” ë™ë£Œì˜ˆìš”.',
        userInfo: userInfoStr,
        workContext: workContextStr, // ğŸ”¥ ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        ragContext: fullIdentityInfo, // ğŸ”¥ RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        roomName: roomContext?.roomName || 'ì±„íŒ…ë°©',
        participants: roomContext?.participantNames?.join(', ') || userName,
        chatHistory: formattedHistory,
        userMessage,
      })
    }

    // deepseek-r1 ëª¨ë¸ì˜ <think> íƒœê·¸ ì œê±°
    const cleanResponse = response.replace(/<think>[\s\S]*?<\/think>\s*/g, '').trim()
    return cleanResponse || response
  } catch (error: any) {
    console.error(`[AgentChat] Error with ${provider}/${model}:`)
    console.error('Error name:', error?.name)
    console.error('Error message:', error?.message)
    console.error('Error cause:', error?.cause)
    throw error
  }
}

// íšŒì˜ ì²¨ë¶€ ìë£Œ íƒ€ì…
interface MeetingAttachment {
  name: string
  content: string
  type: string
}

// ì—ì´ì „íŠ¸ ê°„ ëŒ€í™” ìƒì„± (ë¯¸íŒ… ëª¨ë“œ)
export async function generateAgentMeetingResponse(
  agent: {
    id: string
    name: string
    description?: string
    capabilities?: string[]
    llm_provider?: string | null
    model?: string | null
    temperature?: number | null
    config?: any
  },
  topic: string,
  previousMessages: any[] = [],
  otherAgents: { name: string; role: string }[] = [],
  meetingAttachments?: MeetingAttachment[] | null
): Promise<string> {
  // LLM ì„¤ì • - DBì˜ llm_provider, model í•„ë“œ ìš°ì„  ì‚¬ìš©
  const provider = (agent.llm_provider || agent.config?.llm_provider || 'ollama') as LLMProvider
  const model = agent.model || agent.config?.llm_model || getDefaultModel(provider)

  const llmConfig: LLMConfig = {
    provider,
    model,
    temperature: agent.temperature ?? 0.8, // ë†’ì„ - ì—´ë¤ í† ë¡ ì„ ìœ„í•´ ë‹¤ì–‘í•œ ë°˜ì‘
  }

  console.log(`[AgentMeeting] ${agent.name} using ${provider}/${model}`)

  const llm = createLLM(llmConfig)

  const meetingPrompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(`
ë‹¹ì‹ ì€ "{agentName}"ì´ì—ìš”. ì§€ê¸ˆ íŒ€ íšŒì˜ì—ì„œ **ì—´ë¤ í† ë¡ ** ì¤‘ì…ë‹ˆë‹¤!
{agentDescription}

## ğŸ”¥ ì˜¤ëŠ˜ì˜ í† ë¡  ì£¼ì œ
{topic}

{attachmentsSection}

## ì°¸ì„ì
{otherParticipants}

## ì§€ê¸ˆê¹Œì§€ ë…¼ì˜ ë‚´ìš©
{discussion}

## ğŸ­ í† ë¡  ìŠ¤íƒ€ì¼ - ì—´ì •ì ìœ¼ë¡œ!

### 1. ìì‹ ì˜ ì…ì¥ì„ ê°•í•˜ê²Œ í‘œí˜„
- ë‹¹ì‹ ë§Œì˜ **ëª…í™•í•œ ì˜ê²¬**ì„ ê°€ì§€ì„¸ìš”
- "ì €ëŠ” í™•ì‹¤íˆ ~ë¼ê³  ë´…ë‹ˆë‹¤", "ì•„ë‹ˆìš”, ì €ëŠ” ë‹¤ë¥´ê²Œ ìƒê°í•´ìš”"
- ì• ë§¤í•˜ê²Œ ë§í•˜ì§€ ë§ê³  **ì…ì¥ì„ ë¶„ëª…íˆ**

### 2. ë°˜ë°•í•˜ê³  ë„ì „í•˜ê¸°
- ë‹¤ë¥¸ ì‚¬ëŒ ì˜ê²¬ì— **ë™ì˜ë§Œ í•˜ì§€ ë§ˆì„¸ìš”**
- "ê·¼ë° ê·¸ê±´ ì¢€...", "ê·¸ ë¶€ë¶„ì€ ë™ì˜ ì•ˆ ë¼ìš”", "ì ê¹, ê·¸ê²Œ ë§ë‚˜ìš”?"
- ë…¼ë¦¬ì ìœ¼ë¡œ **ë°˜ë°•í•˜ê³  ëŒ€ì•ˆ ì œì‹œ**
- ìƒëŒ€ë°© ì£¼ì¥ì˜ í—ˆì ì„ ì§€ì í•˜ì„¸ìš”

### 3. ê°ì • í‘œí˜„ OK
- í™•ì‹ : "ì´ê±´ ì§„ì§œ í™•ì‹¤í•´ìš”!", "100% ì´ê²Œ ë§ì•„ìš”"
- ì˜ë¬¸: "ì—ì´~ ê·¸ê±´ ì¢€...", "ê¸€ì„ìš”, ê³¼ì—°?"
- ë™ì˜: "ì˜¤ ê·¸ê±´ ì§„ì§œ ì¢‹ì€ í¬ì¸íŠ¸!", "ì•„ ë§ë‹¤ ê·¸ê±°!"
- ë°˜ëŒ€: "ì•„ë‹ˆì•„ë‹ˆ, ê·¸ê²Œ ì•„ë‹ˆë¼", "ìŒ... ì €ëŠ” ë°˜ëŒ€ì˜ˆìš”"

### 4. êµ¬ì²´ì  ê·¼ê±°
- ë¹ˆë§ ê¸ˆì§€! **êµ¬ì²´ì ì¸ ì´ìœ **ì™€ í•¨ê»˜ ë§í•˜ê¸°
- ìˆ«ì, ì‚¬ë¡€, ê²½í—˜ ì–¸ê¸‰í•˜ê¸°
- "ì™œëƒí•˜ë©´...", "ì˜ˆë¥¼ ë“¤ì–´...", "ì‹¤ì œë¡œ ì €ë²ˆì—..."

### 5. ì§§ì§€ë§Œ ê°•ë ¬í•˜ê²Œ (1-3ë¬¸ì¥)
- í•œ ë²ˆì— í•œ í¬ì¸íŠ¸ë§Œ!
- í•µì‹¬ë§Œ ê°•í•˜ê²Œ ë§í•˜ê³  ë

## ğŸ¯ í† ë¡  íŒ¨í„´ ì˜ˆì‹œ
- ë°˜ë°•: "ê·¼ë° ê·¸ê±´ í˜„ì‹¤ì ìœ¼ë¡œ ì–´ë µì§€ ì•Šì•„ìš”? ì €ë²ˆì—ë„ ë¹„ìŠ·í•˜ê²Œ í–ˆë‹¤ê°€..."
- ëŒ€ì•ˆ: "ê·¸ê²ƒë³´ë‹¨ ì°¨ë¼ë¦¬ ì´ë ‡ê²Œ í•˜ëŠ” ê²Œ ë‚«ì§€ ì•Šì„ê¹Œìš”?"
- ë³´ê°•: "ê±°ê¸°ì— ì¶”ê°€ë¡œ, ì´ëŸ° ì ë„ ê³ ë ¤í•´ì•¼ í•´ìš”"
- ì§ˆë¬¸: "ê·¼ë° ê·¸ëŸ¬ë©´ ì´ ë¶€ë¶„ì€ ì–´ë–»ê²Œ í•  ê±´ë°ìš”?"
- ì£¼ì¥: "ì €ëŠ” í™•ì‹¤íˆ Aì•ˆì´ ë§ë‹¤ê³  ë´ìš”. ì™œëƒí•˜ë©´..."

## ğŸš« ì ˆëŒ€ ê¸ˆì§€
- âŒ "ì¢‹ì€ ê²ƒ ê°™ì•„ìš”", "ë™ì˜í•´ìš”", "ë„¤ë„¤" ê°™ì€ ë¹ˆ ë™ì¡°
- âŒ ì¸ì‚¬, ì•ˆë¶€ (ì´ë¯¸ í† ë¡  ì‹œì‘ë¨)
- âŒ "í¥ë¯¸ë¡­ë„¤ìš”", "ì¬ë°Œë„¤ìš”" ê°™ì€ ë¹ˆ ë¦¬ì•¡ì…˜
- âŒ ëª¨ë“  ì˜ê²¬ì— ê·¸ëƒ¥ ìˆ˜ê¸í•˜ê¸°
- âŒ ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” ì¡ë‹´
- âŒ ë„ˆë¬´ ê¸´ ë°œì–¸ (3ë¬¸ì¥ ì´ˆê³¼)
`),
    HumanMessagePromptTemplate.fromTemplate('í† ë¡  ì£¼ì œì— ëŒ€í•´ ë‹¹ì‹ ë§Œì˜ ê°•í•œ ì˜ê²¬ì„ ë§í•´ì£¼ì„¸ìš”. í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ì˜ê²¬ì— ë°˜ë°•í•˜ì„¸ìš”.'),
  ])

  const chain = meetingPrompt.pipe(llm).pipe(new StringOutputParser())

  try {
    // ì²¨ë¶€ ìë£Œ ì„¹ì…˜ ìƒì„±
    let attachmentsSection = ''
    if (meetingAttachments && meetingAttachments.length > 0) {
      attachmentsSection = `## ğŸ“ íšŒì˜ ì²¨ë¶€ ìë£Œ (ë°˜ë“œì‹œ ì°¸ê³ í•˜ì„¸ìš”!)
ì•„ë˜ëŠ” ì´ íšŒì˜ì—ì„œ ì°¸ê³ í•´ì•¼ í•  ìë£Œì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í† ë¡ í•˜ì„¸ìš”.

${meetingAttachments.map((att, idx) => `### ìë£Œ ${idx + 1}: ${att.name}
\`\`\`
${att.content.substring(0, 5000)}${att.content.length > 5000 ? '\n... (ë‚´ìš© ì¼ë¶€ ìƒëµ)' : ''}
\`\`\`
`).join('\n')}`
    }

    const response = await chain.invoke({
      agentName: agent.name,
      agentDescription: agent.description || '',
      topic,
      attachmentsSection,
      otherParticipants: otherAgents.map((a) => `- ${a.name} (${a.role})`).join('\n'),
      discussion: formatChatHistory(previousMessages),
    })

    // deepseek-r1 ëª¨ë¸ì˜ <think> íƒœê·¸ ì œê±°
    const cleanResponse = response.replace(/<think>[\s\S]*?<\/think>\s*/g, '').trim()
    return cleanResponse || response
  } catch (error) {
    console.error(`[AgentMeeting] Error with ${provider}/${model}:`, error)
    throw error
  }
}

// ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë‚´ë³´ë‚´ê¸°
export { AVAILABLE_MODELS }
