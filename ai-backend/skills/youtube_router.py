"""
YouTube Transcript API Router
FastAPI ë¼ìš°í„°ë¡œ YouTube ìë§‰ ì¶”ì¶œ ê¸°ëŠ¥ ì œê³µ
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import os
import subprocess
import json
import tempfile

router = APIRouter()


class YouTubeRequest(BaseModel):
    url: str
    languages: List[str] = ['ko', 'en']
    generate_summary: bool = True
    proxy: Optional[str] = None  # í”„ë¡ì‹œ URL (ì˜ˆ: 'socks5://127.0.0.1:1080')
    use_supadata: bool = False  # Supadata API ì‚¬ìš© ì—¬ë¶€


class TranscriptItem(BaseModel):
    timestamp: str
    text: str
    start: float
    duration: float


class VideoInfo(BaseModel):
    id: str
    title: str
    channel: str
    thumbnail: str
    date: str
    views: str


class TimelineItem(BaseModel):
    title: str
    timestamp: str
    content: str
    details: List[str]


class Summary(BaseModel):
    threeLine: List[str]
    tableOfContents: List[str]
    timeline: List[TimelineItem]
    keyPoints: Optional[List[str]] = None  # í•µì‹¬ìš”ì•½ (5-10ê°œ í¬ì¸íŠ¸)
    blogPost: Optional[str] = None  # ë¸”ë¡œê·¸ ê¸€


class YouTubeResponse(BaseModel):
    success: bool
    videoInfo: Optional[VideoInfo] = None
    transcript: Optional[List[TranscriptItem]] = None
    fullText: Optional[str] = None
    summary: Optional[Summary] = None
    error: Optional[str] = None


def get_transcript_with_supadata(video_id: str, languages: List[str] = ['ko', 'en']) -> dict:
    """Supadata APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ ì¶”ì¶œ (ìœ ë£Œ ì„œë¹„ìŠ¤, ì•ˆì •ì )

    í™˜ê²½ë³€ìˆ˜ SUPADATA_API_KEY í•„ìš”
    ê°€ì…: https://supadata.ai
    """
    import requests

    api_key = os.environ.get('SUPADATA_API_KEY')
    if not api_key:
        return {'success': False, 'error': 'SUPADATA_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. https://supadata.ai ì—ì„œ ë¬´ë£Œ ê°€ì…í•˜ì„¸ìš”.'}

    try:
        lang = languages[0] if languages else 'en'
        url = f"https://api.supadata.ai/v1/youtube/transcript?url=https://www.youtube.com/watch?v={video_id}&lang={lang}"

        resp = requests.get(url, headers={'x-api-key': api_key}, timeout=30)

        if resp.status_code == 401:
            return {'success': False, 'error': 'Supadata API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'}

        if resp.status_code != 200:
            return {'success': False, 'error': f'Supadata API ì˜¤ë¥˜: {resp.status_code}'}

        data = resp.json()

        # ì‘ë‹µ íŒŒì‹±
        transcript = []
        full_text = []

        if 'content' in data:
            for item in data['content']:
                start = item.get('offset', 0) / 1000  # ms to seconds
                text = item.get('text', '').strip()

                if text:
                    mins = int(start // 60)
                    secs = int(start % 60)
                    transcript.append({
                        'start': start,
                        'duration': item.get('duration', 0) / 1000,
                        'text': text,
                        'timestamp': f"{mins:02d}:{secs:02d}"
                    })
                    full_text.append(text)

        if not transcript:
            return {'success': False, 'error': 'ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}

        # ë¹„ë””ì˜¤ ì •ë³´ - Supadataì—ì„œ ì œê³µí•˜ê±°ë‚˜ yt-dlpë¡œ ê°€ì ¸ì˜¤ê¸°
        title = data.get('title', '')
        channel = ''
        views = ''
        upload_date = ''

        # ì œëª©ì´ ì—†ìœ¼ë©´ yt-dlpë¡œ ì˜ìƒ ì •ë³´ë§Œ ê°€ì ¸ì˜¤ê¸°
        if not title:
            try:
                import subprocess
                import sys
                cmd = [sys.executable, '-m', 'yt_dlp', '--skip-download', '-j', f'https://www.youtube.com/watch?v={video_id}']
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                if result.returncode == 0 and result.stdout:
                    import json as json_module
                    for line in reversed(result.stdout.strip().split('\n')):
                        if line.strip().startswith('{'):
                            try:
                                info = json_module.loads(line)
                                title = info.get('title', '')
                                channel = info.get('channel', info.get('uploader', ''))
                                views = f"{info.get('view_count', 0):,}íšŒ" if info.get('view_count') else ''
                                upload_date = info.get('upload_date', '')[:10] if info.get('upload_date') else ''
                                break
                            except:
                                pass
            except Exception as e:
                print(f"yt-dlp video info error: {e}")

        video_info = {
            'id': video_id,
            'title': title,
            'channel': channel,
            'thumbnail': f'https://img.youtube.com/vi/{video_id}/maxresdefault.jpg',
            'date': upload_date,
            'views': views
        }

        return {
            'success': True,
            'videoInfo': video_info,
            'transcript': transcript,
            'fullText': ' '.join(full_text)
        }

    except Exception as e:
        return {'success': False, 'error': f'Supadata API ì˜¤ë¥˜: {str(e)}'}


def get_transcript_with_proxy(video_id: str, languages: List[str] = ['ko', 'en']) -> dict:
    """í”„ë¡ì‹œ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ìë§‰ ì¶”ì¶œ (Bright Data, ScraperAPI ë“±)

    í™˜ê²½ë³€ìˆ˜:
    - BRIGHTDATA_PROXY: Bright Data í”„ë¡ì‹œ URL
    - SCRAPERAPI_KEY: ScraperAPI í‚¤
    """
    import requests
    import re

    # Bright Data í”„ë¡ì‹œ
    brightdata_proxy = os.environ.get('BRIGHTDATA_PROXY')
    if brightdata_proxy:
        try:
            url = f'https://www.youtube.com/watch?v={video_id}'
            proxies = {'http': brightdata_proxy, 'https': brightdata_proxy}

            resp = requests.get(url, proxies=proxies, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }, timeout=30)

            if resp.status_code == 200 and 'captionTracks' in resp.text:
                # ìë§‰ URL ì¶”ì¶œ
                match = re.search(r'ytInitialPlayerResponse\s*=\s*(\{.+?\});', resp.text)
                if match:
                    import json
                    player_response = json.loads(match.group(1))
                    captions = player_response.get('captions', {}).get('playerCaptionsTracklistRenderer', {}).get('captionTracks', [])

                    if captions:
                        # ì–¸ì–´ ì„ íƒ
                        selected = None
                        for lang in languages:
                            for cap in captions:
                                if cap.get('languageCode') == lang:
                                    selected = cap
                                    break
                            if selected:
                                break
                        if not selected:
                            selected = captions[0]

                        # ìë§‰ ë‹¤ìš´ë¡œë“œ
                        cap_resp = requests.get(selected['baseUrl'], proxies=proxies, headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                        }, timeout=30)

                        if cap_resp.status_code == 200:
                            # XML íŒŒì‹±
                            transcript = []
                            text_matches = re.findall(r'<text start="([^"]+)"[^>]*>([^<]*)</text>', cap_resp.text)
                            for start, text in text_matches:
                                text = text.replace('&amp;', '&').replace('&#39;', "'").replace('&quot;', '"').replace('&lt;', '<').replace('&gt;', '>')
                                if text.strip():
                                    start_sec = float(start)
                                    mins = int(start_sec // 60)
                                    secs = int(start_sec % 60)
                                    transcript.append({
                                        'start': start_sec,
                                        'duration': 0,
                                        'text': text.strip(),
                                        'timestamp': f"{mins:02d}:{secs:02d}"
                                    })

                            if transcript:
                                # ë¹„ë””ì˜¤ ì •ë³´
                                video_info = {
                                    'id': video_id,
                                    'title': player_response.get('videoDetails', {}).get('title', ''),
                                    'channel': player_response.get('videoDetails', {}).get('author', ''),
                                    'thumbnail': f'https://img.youtube.com/vi/{video_id}/maxresdefault.jpg',
                                    'date': '',
                                    'views': f"{int(player_response.get('videoDetails', {}).get('viewCount', 0)):,}íšŒ"
                                }

                                return {
                                    'success': True,
                                    'videoInfo': video_info,
                                    'transcript': transcript,
                                    'fullText': ' '.join([t['text'] for t in transcript])
                                }
        except Exception as e:
            print(f"Bright Data proxy error: {e}")

    # ScraperAPI
    scraperapi_key = os.environ.get('SCRAPERAPI_KEY')
    if scraperapi_key:
        try:
            url = f'http://api.scraperapi.com?api_key={scraperapi_key}&url=https://www.youtube.com/watch?v={video_id}'
            resp = requests.get(url, timeout=60)

            if resp.status_code == 200 and 'captionTracks' in resp.text:
                # ìœ„ì™€ ë™ì¼í•œ íŒŒì‹± ë¡œì§
                match = re.search(r'ytInitialPlayerResponse\s*=\s*(\{.+?\});', resp.text)
                if match:
                    import json
                    player_response = json.loads(match.group(1))
                    captions = player_response.get('captions', {}).get('playerCaptionsTracklistRenderer', {}).get('captionTracks', [])

                    if captions:
                        selected = captions[0]
                        for lang in languages:
                            for cap in captions:
                                if cap.get('languageCode') == lang:
                                    selected = cap
                                    break

                        cap_url = f'http://api.scraperapi.com?api_key={scraperapi_key}&url={selected["baseUrl"]}'
                        cap_resp = requests.get(cap_url, timeout=60)

                        if cap_resp.status_code == 200:
                            transcript = []
                            text_matches = re.findall(r'<text start="([^"]+)"[^>]*>([^<]*)</text>', cap_resp.text)
                            for start, text in text_matches:
                                text = text.replace('&amp;', '&').replace('&#39;', "'").replace('&quot;', '"')
                                if text.strip():
                                    start_sec = float(start)
                                    mins = int(start_sec // 60)
                                    secs = int(start_sec % 60)
                                    transcript.append({
                                        'start': start_sec,
                                        'duration': 0,
                                        'text': text.strip(),
                                        'timestamp': f"{mins:02d}:{secs:02d}"
                                    })

                            if transcript:
                                video_info = {
                                    'id': video_id,
                                    'title': player_response.get('videoDetails', {}).get('title', ''),
                                    'channel': player_response.get('videoDetails', {}).get('author', ''),
                                    'thumbnail': f'https://img.youtube.com/vi/{video_id}/maxresdefault.jpg',
                                    'date': '',
                                    'views': ''
                                }

                                return {
                                    'success': True,
                                    'videoInfo': video_info,
                                    'transcript': transcript,
                                    'fullText': ' '.join([t['text'] for t in transcript])
                                }
        except Exception as e:
            print(f"ScraperAPI error: {e}")

    return {'success': False, 'error': 'í”„ë¡ì‹œ ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤'}


def get_transcript_with_ytdlp(video_id: str, languages: List[str] = ['ko', 'en'], proxy: str = None) -> dict:
    """yt-dlpë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ ì¶”ì¶œ (ì§ì ‘ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë°©ì‹)

    Args:
        video_id: YouTube ë¹„ë””ì˜¤ ID
        languages: ìë§‰ ì–¸ì–´ ìš°ì„ ìˆœìœ„
        proxy: í”„ë¡ì‹œ URL (ì˜ˆ: 'socks5://127.0.0.1:1080' ë˜ëŠ” 'http://proxy:8080')
    """
    import re
    import sys
    import glob

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í”„ë¡ì‹œ ì„¤ì • í™•ì¸
    if not proxy:
        proxy = os.environ.get('YOUTUBE_PROXY') or os.environ.get('HTTP_PROXY') or os.environ.get('BRIGHTDATA_PROXY')

    try:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = tempfile.mkdtemp(prefix='yt_')
        output_template = os.path.join(output_dir, '%(id)s')

        # 1. ë¨¼ì € ì˜ìƒ ì •ë³´ë§Œ ê°€ì ¸ì˜¤ê¸°
        cmd = [sys.executable, '-m', 'yt_dlp', '--skip-download', '-j']
        if proxy:
            cmd.extend(['--proxy', proxy])
        cmd.append(f'https://www.youtube.com/watch?v={video_id}')

        info_result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=60
        )

        video_info = {
            'id': video_id,
            'title': '',
            'channel': '',
            'thumbnail': f'https://img.youtube.com/vi/{video_id}/maxresdefault.jpg',
            'date': '',
            'views': '',
        }

        if info_result.returncode == 0 and info_result.stdout:
            stdout_lines = info_result.stdout.strip().split('\n')
            for line in reversed(stdout_lines):
                if line.strip().startswith('{'):
                    try:
                        info = json.loads(line)
                        video_info = {
                            'id': video_id,
                            'title': info.get('title', ''),
                            'channel': info.get('channel', info.get('uploader', '')),
                            'thumbnail': info.get('thumbnail', f'https://img.youtube.com/vi/{video_id}/maxresdefault.jpg'),
                            'date': info.get('upload_date', '')[:10] if info.get('upload_date') else '',
                            'views': f"{info.get('view_count', 0):,}íšŒ" if info.get('view_count') else '',
                        }
                    except:
                        pass
                    break

        # 2. ìë§‰ íŒŒì¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (yt-dlpê°€ ì²˜ë¦¬)
        lang_opts = ','.join(languages)
        sub_cmd = [
            sys.executable, '-m', 'yt_dlp',
            '--skip-download',
            '--write-sub',
            '--write-auto-sub',
            '--sub-lang', lang_opts,
            '--sub-format', 'vtt/srt/best',
            '--convert-subs', 'vtt',
        ]
        if proxy:
            sub_cmd.extend(['--proxy', proxy])
        sub_cmd.extend(['-o', output_template, f'https://www.youtube.com/watch?v={video_id}'])

        sub_result = subprocess.run(
            sub_cmd,
            capture_output=True,
            text=True,
            timeout=120,
            cwd=output_dir
        )

        # 3. ë‹¤ìš´ë¡œë“œëœ ìë§‰ íŒŒì¼ ì°¾ê¸°
        sub_files = glob.glob(os.path.join(output_dir, '*.vtt')) + \
                    glob.glob(os.path.join(output_dir, '*.srt'))

        if not sub_files:
            # ë””ë ‰í† ë¦¬ ì •ë¦¬
            import shutil
            shutil.rmtree(output_dir, ignore_errors=True)
            return {'success': False, 'error': 'ìë§‰ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ì˜ìƒì— ìë§‰ì´ ì—†ê±°ë‚˜ YouTubeê°€ ì°¨ë‹¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'}

        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
        sub_file = max(sub_files, key=os.path.getmtime)

        with open(sub_file, 'r', encoding='utf-8') as f:
            sub_content = f.read()

        # ë””ë ‰í† ë¦¬ ì •ë¦¬
        import shutil
        shutil.rmtree(output_dir, ignore_errors=True)

        # 4. VTT/SRT íŒŒì‹±
        transcript = []

        if 'WEBVTT' in sub_content or sub_file.endswith('.vtt'):
            # VTT í˜•ì‹ íŒŒì‹±
            # íŒ¨í„´: 00:00:00.000 --> 00:00:00.000
            pattern = r'(\d{2}:\d{2}:\d{2}[.,]\d{3})\s*-->\s*\d{2}:\d{2}:\d{2}[.,]\d{3}[^\n]*\n((?:(?!\d{2}:\d{2}:\d{2}).*\n?)*)'
            matches = re.findall(pattern, sub_content)

            for timestamp, text in matches:
                # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±
                timestamp = timestamp.replace(',', '.')
                parts = timestamp.split(':')
                hrs = int(parts[0])
                mins = int(parts[1])
                secs = float(parts[2])
                start = hrs * 3600 + mins * 60 + secs

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
                text = re.sub(r'\{[^}]+\}', '', text)  # ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                text = text.strip()

                if text and not text.startswith('WEBVTT'):
                    transcript.append({
                        'start': start,
                        'duration': 0,
                        'text': text,
                        'timestamp': f"{mins:02d}:{int(secs):02d}" if hrs == 0 else f"{hrs:02d}:{mins:02d}:{int(secs):02d}"
                    })
        else:
            # SRT í˜•ì‹ íŒŒì‹±
            pattern = r'(\d{2}:\d{2}:\d{2}[.,]\d{3})\s*-->\s*\d{2}:\d{2}:\d{2}[.,]\d{3}\n((?:(?!\d+\n\d{2}:\d{2}:\d{2}).*\n?)*)'
            matches = re.findall(pattern, sub_content)

            for timestamp, text in matches:
                timestamp = timestamp.replace(',', '.')
                parts = timestamp.split(':')
                hrs = int(parts[0])
                mins = int(parts[1])
                secs = float(parts[2])
                start = hrs * 3600 + mins * 60 + secs

                text = re.sub(r'<[^>]+>', '', text)
                text = text.strip()

                if text:
                    transcript.append({
                        'start': start,
                        'duration': 0,
                        'text': text,
                        'timestamp': f"{mins:02d}:{int(secs):02d}" if hrs == 0 else f"{hrs:02d}:{mins:02d}:{int(secs):02d}"
                    })

        # ì¤‘ë³µ ì œê±° (ì—°ì†ëœ ê°™ì€ í…ìŠ¤íŠ¸)
        deduplicated = []
        prev_text = ''
        for item in transcript:
            if item['text'] != prev_text:
                deduplicated.append(item)
                prev_text = item['text']
        transcript = deduplicated

        if not transcript:
            return {'success': False, 'error': 'ìë§‰ íŒŒì‹± ì‹¤íŒ¨'}

        return {
            'success': True,
            'videoInfo': video_info,
            'transcript': transcript,
            'fullText': ' '.join([t['text'] for t in transcript])
        }

    except subprocess.TimeoutExpired:
        return {'success': False, 'error': 'yt-dlp íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)'}
    except FileNotFoundError:
        return {'success': False, 'error': 'yt-dlpê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def generate_summary_with_ai(full_text: str, title: str) -> dict:
    """AIë¡œ ìš”ì•½ ìƒì„± (í•µì‹¬ìš”ì•½ + ë¸”ë¡œê·¸ ê¸€ í¬í•¨)"""
    import requests

    # Grok API ë¨¼ì € ì‹œë„
    grok_api_key = os.environ.get('XAI_API_KEY')
    if grok_api_key:
        try:
            prompt = f'''ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê¼¼ê¼¼íˆ ì½ê³  ìƒì„¸í•œ íƒ€ì„ë¼ì¸ê³¼ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.

ì˜ìƒ ì œëª©: {title}

ìŠ¤í¬ë¦½íŠ¸ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨):
{full_text[:20000]}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš” (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ):

{{
    "threeLine": [
        "í•µì‹¬ ë©”ì‹œì§€ 1: ì˜ìƒì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì£¼ì¥ì´ë‚˜ ì •ë³´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ (ì˜ˆ: íŠ¹ì • ìˆ˜ì¹˜, ì‚¬ë¡€, ì¸ìš©ë¬¸ í¬í•¨)",
        "í•µì‹¬ ë©”ì‹œì§€ 2: ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ",
        "í•µì‹¬ ë©”ì‹œì§€ 3: ì„¸ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ"
    ],
    "keyPoints": [
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸ (ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ í¬í•¨)",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸"
    ],
    "tableOfContents": [
        "1. ì²« ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "2. ë‘ ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "3. ì„¸ ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "4. ë„¤ ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "5. ë‹¤ì„¯ ë²ˆì§¸ ëŒ€ì£¼ì œ"
    ],
    "timeline": [
        {{
            "title": "ğŸ¬ ì¸íŠ¸ë¡œ: [êµ¬ì²´ì ì¸ ë„ì…ë¶€ ë‚´ìš©]",
            "timestamp": "00:00",
            "content": "ì´ ì„¹ì…˜ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì„ 3-5ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…. ë°œí™”ìê°€ ë§í•œ í•µì‹¬ ë‚´ìš©, ì˜ˆì‹œ, ì£¼ì¥ì„ êµ¬ì²´ì ìœ¼ë¡œ í¬í•¨.",
            "details": [
                "â€¢ ì²« ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: êµ¬ì²´ì ì¸ ë‚´ìš©ì´ë‚˜ ì˜ˆì‹œ",
                "â€¢ ë‘ ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: ì–¸ê¸‰ëœ ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€",
                "â€¢ ì„¸ ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: í•µì‹¬ ì¸ìš©ì´ë‚˜ ì£¼ì¥",
                "â€¢ ë„¤ ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: ì¶”ê°€ ì •ë³´"
            ]
        }},
        {{
            "title": "ğŸ“Œ [ë‘ ë²ˆì§¸ ì„¹ì…˜ ì œëª©]",
            "timestamp": "MM:SS",
            "content": "ìƒì„¸ ì„¤ëª…...",
            "details": ["â€¢ ì„¸ë¶€1", "â€¢ ì„¸ë¶€2", "â€¢ ì„¸ë¶€3", "â€¢ ì„¸ë¶€4"]
        }}
    ],
    "blogPost": "ìƒì„¸ ë¸”ë¡œê·¸ ê¸€ (2500ì ì´ìƒ)"
}}

âš ï¸ íƒ€ì„ë¼ì¸ ì‘ì„± ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!!!):
1. ìµœì†Œ 8-12ê°œ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš” (ì˜ìƒ ê¸¸ì´ì— ë¹„ë¡€)
2. timestampëŠ” ìŠ¤í¬ë¦½íŠ¸ì˜ ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•íˆ ê¸°ì…
3. titleì€ ì´ëª¨ì§€ + êµ¬ì²´ì ì¸ ì œëª© (ì˜ˆ: "ğŸ”¥ AI ì—ì´ì „íŠ¸ê°€ ë°”ê¿€ ì—…ë¬´ ìë™í™”ì˜ ë¯¸ë˜")
4. contentëŠ” 3-5ë¬¸ì¥ìœ¼ë¡œ í•´ë‹¹ êµ¬ê°„ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸íˆ ì„¤ëª…
5. detailsëŠ” 4-6ê°œì˜ êµ¬ì²´ì ì¸ í¬ì¸íŠ¸ (ì˜ˆì‹œ, ìˆ˜ì¹˜, ì¸ìš©ë¬¸ í¬í•¨)
6. ê° ì„¹ì…˜ì´ ë¬´ìŠ¨ ë‚´ìš©ì¸ì§€ ì½ëŠ” ì‚¬ëŒì´ ì˜ìƒì„ ì•ˆ ë´ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±

ğŸ“ ì „ì²´ ì‘ì„± ê·œì¹™:
- ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±
- ì¶”ìƒì ì¸ í‘œí˜„ ê¸ˆì§€, êµ¬ì²´ì ì¸ ì •ë³´ë§Œ ì‘ì„±
- "~ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤" ê°™ì€ ë©”íƒ€ ì„¤ëª… ê¸ˆì§€
- ì‹¤ì œ ì˜ìƒì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ ì‘ì„±
- blogPostëŠ” 2500ì ì´ìƒ, ë…ìê°€ ì˜ìƒì„ ì•ˆ ë´ë„ ë  ì •ë„ë¡œ ìƒì„¸íˆ'''

            response = requests.post(
                'https://api.x.ai/v1/chat/completions',
                headers={
                    'Authorization': f'Bearer {grok_api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': 'grok-3-mini-fast-beta',
                    'messages': [{'role': 'user', 'content': prompt}],
                    'temperature': 0.4,
                    'max_tokens': 8000,
                },
                timeout=120
            )

            if response.ok:
                result = response.json()
                text = result['choices'][0]['message']['content']

                # JSON ì¶”ì¶œ
                import re
                json_str = text.strip()
                if json_str.startswith('```'):
                    json_str = re.sub(r'```(?:json)?\n?', '', json_str).strip()

                return json.loads(json_str)
        except Exception as e:
            print(f"Grok API error: {e}")

    # Gemini API ì‹œë„
    google_api_key = os.environ.get('GOOGLE_API_KEY')
    if google_api_key:
        try:
            prompt = f'''ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê¼¼ê¼¼íˆ ì½ê³  ìƒì„¸í•œ íƒ€ì„ë¼ì¸ê³¼ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.

ì˜ìƒ ì œëª©: {title}

ìŠ¤í¬ë¦½íŠ¸ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨):
{full_text[:20000]}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš” (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ):

{{
    "threeLine": [
        "í•µì‹¬ ë©”ì‹œì§€ 1: ì˜ìƒì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì£¼ì¥ì´ë‚˜ ì •ë³´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ (ì˜ˆ: íŠ¹ì • ìˆ˜ì¹˜, ì‚¬ë¡€, ì¸ìš©ë¬¸ í¬í•¨)",
        "í•µì‹¬ ë©”ì‹œì§€ 2: ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ",
        "í•µì‹¬ ë©”ì‹œì§€ 3: ì„¸ ë²ˆì§¸ë¡œ ì¤‘ìš”í•œ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ"
    ],
    "keyPoints": [
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸ (ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ í¬í•¨)",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸",
        "ğŸ’¡ [ì£¼ì œ] êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸"
    ],
    "tableOfContents": [
        "1. ì²« ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "2. ë‘ ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "3. ì„¸ ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "4. ë„¤ ë²ˆì§¸ ëŒ€ì£¼ì œ",
        "5. ë‹¤ì„¯ ë²ˆì§¸ ëŒ€ì£¼ì œ"
    ],
    "timeline": [
        {{
            "title": "ğŸ¬ ì¸íŠ¸ë¡œ: [êµ¬ì²´ì ì¸ ë„ì…ë¶€ ë‚´ìš©]",
            "timestamp": "00:00",
            "content": "ì´ ì„¹ì…˜ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì„ 3-5ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…. ë°œí™”ìê°€ ë§í•œ í•µì‹¬ ë‚´ìš©, ì˜ˆì‹œ, ì£¼ì¥ì„ êµ¬ì²´ì ìœ¼ë¡œ í¬í•¨.",
            "details": [
                "â€¢ ì²« ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: êµ¬ì²´ì ì¸ ë‚´ìš©ì´ë‚˜ ì˜ˆì‹œ",
                "â€¢ ë‘ ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: ì–¸ê¸‰ëœ ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€",
                "â€¢ ì„¸ ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: í•µì‹¬ ì¸ìš©ì´ë‚˜ ì£¼ì¥",
                "â€¢ ë„¤ ë²ˆì§¸ ì„¸ë¶€ í¬ì¸íŠ¸: ì¶”ê°€ ì •ë³´"
            ]
        }},
        {{
            "title": "ğŸ“Œ [ë‘ ë²ˆì§¸ ì„¹ì…˜ ì œëª©]",
            "timestamp": "MM:SS",
            "content": "ìƒì„¸ ì„¤ëª…...",
            "details": ["â€¢ ì„¸ë¶€1", "â€¢ ì„¸ë¶€2", "â€¢ ì„¸ë¶€3", "â€¢ ì„¸ë¶€4"]
        }}
    ],
    "blogPost": "ìƒì„¸ ë¸”ë¡œê·¸ ê¸€ (2500ì ì´ìƒ)"
}}

âš ï¸ íƒ€ì„ë¼ì¸ ì‘ì„± ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!!!):
1. ìµœì†Œ 8-12ê°œ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš” (ì˜ìƒ ê¸¸ì´ì— ë¹„ë¡€)
2. timestampëŠ” ìŠ¤í¬ë¦½íŠ¸ì˜ ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•íˆ ê¸°ì…
3. titleì€ ì´ëª¨ì§€ + êµ¬ì²´ì ì¸ ì œëª© (ì˜ˆ: "ğŸ”¥ AI ì—ì´ì „íŠ¸ê°€ ë°”ê¿€ ì—…ë¬´ ìë™í™”ì˜ ë¯¸ë˜")
4. contentëŠ” 3-5ë¬¸ì¥ìœ¼ë¡œ í•´ë‹¹ êµ¬ê°„ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸íˆ ì„¤ëª…
5. detailsëŠ” 4-6ê°œì˜ êµ¬ì²´ì ì¸ í¬ì¸íŠ¸ (ì˜ˆì‹œ, ìˆ˜ì¹˜, ì¸ìš©ë¬¸ í¬í•¨)
6. ê° ì„¹ì…˜ì´ ë¬´ìŠ¨ ë‚´ìš©ì¸ì§€ ì½ëŠ” ì‚¬ëŒì´ ì˜ìƒì„ ì•ˆ ë´ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±

ğŸ“ ì „ì²´ ì‘ì„± ê·œì¹™:
- ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±
- ì¶”ìƒì ì¸ í‘œí˜„ ê¸ˆì§€, êµ¬ì²´ì ì¸ ì •ë³´ë§Œ ì‘ì„±
- "~ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤" ê°™ì€ ë©”íƒ€ ì„¤ëª… ê¸ˆì§€
- ì‹¤ì œ ì˜ìƒì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ ì‘ì„±
- blogPostëŠ” 2500ì ì´ìƒ, ë…ìê°€ ì˜ìƒì„ ì•ˆ ë´ë„ ë  ì •ë„ë¡œ ìƒì„¸íˆ'''

            url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={google_api_key}'
            response = requests.post(url, json={
                'contents': [{'parts': [{'text': prompt}]}],
                'generationConfig': {
                    'temperature': 0.4,
                    'maxOutputTokens': 8000,
                }
            }, timeout=120)

            if response.ok:
                result = response.json()
                text = result['candidates'][0]['content']['parts'][0]['text']

                # JSON ì¶”ì¶œ
                import re
                json_str = text.strip()
                if json_str.startswith('```'):
                    json_str = re.sub(r'```(?:json)?\n?', '', json_str).strip()

                return json.loads(json_str)
        except Exception as e:
            print(f"Gemini API error: {e}")

    return None


@router.post("/transcript", response_model=YouTubeResponse)
async def get_youtube_transcript(request: YouTubeRequest):
    """ìœ íŠœë¸Œ ì˜ìƒì˜ ìë§‰ ì¶”ì¶œ ë° ìš”ì•½"""
    import re

    # URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)',
        r'youtube\.com\/shorts\/([^&\n?#]+)',
    ]

    video_id = None
    for pattern in patterns:
        match = re.search(pattern, request.url)
        if match:
            video_id = match.group(1)
            break

    if not video_id and len(request.url) == 11:
        video_id = request.url

    if not video_id:
        return YouTubeResponse(success=False, error='ìœ íš¨í•œ ìœ íŠœë¸Œ ë§í¬ê°€ ì•„ë‹™ë‹ˆë‹¤')

    # ìë§‰ ì¶”ì¶œ ì‹œë„ (ìš°ì„ ìˆœìœ„: Supadata â†’ í”„ë¡ì‹œ â†’ yt-dlp)
    result = None

    # 1. Supadata API ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì , ìœ ë£Œ)
    if request.use_supadata or os.environ.get('SUPADATA_API_KEY'):
        result = get_transcript_with_supadata(video_id, request.languages)
        if result['success']:
            print(f"âœ“ Got transcript from Supadata API")

    # 2. í”„ë¡ì‹œ ì„œë¹„ìŠ¤ ì‹œë„ (Bright Data, ScraperAPI)
    if not result or not result['success']:
        if os.environ.get('BRIGHTDATA_PROXY') or os.environ.get('SCRAPERAPI_KEY'):
            result = get_transcript_with_proxy(video_id, request.languages)
            if result['success']:
                print(f"âœ“ Got transcript via proxy service")

    # 3. yt-dlp ì‹œë„ (ë¡œì»¬ í™˜ê²½ ë˜ëŠ” VPN ì‚¬ìš© ì‹œ)
    if not result or not result['success']:
        result = get_transcript_with_ytdlp(video_id, request.languages, request.proxy)
        if result['success']:
            print(f"âœ“ Got transcript from yt-dlp")

    if not result['success']:
        error_msg = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
        # IP ì°¨ë‹¨ ê´€ë ¨ ì—ëŸ¬ì¸ ê²½ìš° ì¶”ê°€ ì•ˆë‚´
        if '429' in str(error_msg) or 'ì°¨ë‹¨' in str(error_msg) or 'blocked' in str(error_msg).lower():
            error_msg += '\n\ní•´ê²° ë°©ë²•:\n1. VPN ì‚¬ìš©\n2. ëª‡ ì‹œê°„ í›„ ì¬ì‹œë„\n3. SUPADATA_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì • (https://supadata.ai ë¬´ë£Œ ê°€ì…)'
        return YouTubeResponse(success=False, error=error_msg)

    # ì‘ë‹µ êµ¬ì„±
    response = YouTubeResponse(
        success=True,
        videoInfo=VideoInfo(**result['videoInfo']),
        transcript=[TranscriptItem(**t) for t in result['transcript']],
        fullText=result['fullText']
    )

    # AI ìš”ì•½ ìƒì„±
    if request.generate_summary:
        summary = generate_summary_with_ai(result['fullText'], result['videoInfo']['title'])
        if summary:
            response.summary = Summary(**summary)

    return response


@router.get("/test")
async def test_youtube():
    """YouTube ìŠ¤í‚¬ í…ŒìŠ¤íŠ¸"""
    return {"status": "ok", "message": "YouTube transcript skill is ready"}
